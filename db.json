{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[],"Cache":[{"_id":"source/_posts/1-100素数判断pyhon程序.md","hash":"30a99579fa7cdaaba229d711e239349bf20e66b3","modified":1481379084624},{"_id":"source/_posts/17年要读的书和学习的技能.md","hash":"5c5a9a059e449e5309600c4c93e7c81fc397c8e9","modified":1481379084624},{"_id":"source/_posts/Install-Caffe-on-CentOS.md","hash":"da9ed9b5c630cd62d9c3cef3d49c4ef6eccb9c43","modified":1481379084624},{"_id":"source/_posts/MAC访问你DOCKER容器中的WEB页面.md","hash":"45f818e11c8c77ff60821f24ab3fd7c31341b98f","modified":1481379084624},{"_id":"source/_posts/MergeSort.md","hash":"3a440ca8eae819d3e0c2fa029c76c1036847b4f8","modified":1481379084624},{"_id":"source/_posts/SPARK的宽依赖和窄依赖.md","hash":"da04cdbee0077f1b25eb1e77b131217699fdfe9c","modified":1481379084624},{"_id":"source/_posts/Save-DataFrame-into-a-partitioned-table-of-HIVE.md","hash":"54f114ba67f0ea47b72959f41c55fdef5de04159","modified":1481379084624},{"_id":"source/_posts/Scala-call-by-name-call-by-value.md","hash":"d8a538cd503a63d355f028732c416d4b2597a1cd","modified":1481379084624},{"_id":"source/_posts/ScalaList.md","hash":"4a8ffa182ebd9d273725df66703c72691fa8548e","modified":1481379084624},{"_id":"source/_posts/Spark-2-0-Introduction.md","hash":"5f589230c88480a8d1e62521245a34d64e1e0718","modified":1481379084624},{"_id":"source/_posts/Spark-Window-Operation.md","hash":"f5f0f9899beecb5af0d32cd9df18ce5492c5be53","modified":1481379084624},{"_id":"source/_posts/SparkDataFrameLikeSql.md","hash":"06d1e58704008cbf74df82dd9d2e59edd99c8ae4","modified":1481379396104},{"_id":"source/_posts/SparkPassFunctions.md","hash":"1c56f7acd3f216656f18efeb382ed5e6806459e6","modified":1481379084624},{"_id":"source/_posts/SparkStreamLearning.md","hash":"85b2e607dcd7e5a68bc4a360b82a4dc2bea75448","modified":1481379084624},{"_id":"source/_posts/save-spark-rdd-into-Mysql.md","hash":"4248aa52a30b556faca7952f6a42c88e8219fa44","modified":1481379084624},{"_id":"source/_posts/机器学习相关材料.md","hash":"f54bc4b09c16ed132a69f01e6d227c303245f79b","modified":1481379084624},{"_id":"source/_posts/过拟合的原因.md","hash":"f154fcc55c1dac71fcb4171a1580de16f04d66dd","modified":1481379084624},{"_id":"source/about/index.md","hash":"c015219bd60b9bc0d0c604d1377543744f67a649","modified":1481379084624},{"_id":"source/categories/index.md","hash":"415d779cf75f2d2692c951ed2e4fd8f590545bc7","modified":1481379084625},{"_id":"source/tags/index.md","hash":"23708c34e5f98b7abe4c047bc98182dcb322007b","modified":1481379084625},{"_id":"public/atom.xml","hash":"2e1480346f7a1d4f226b1aa72069223eea4cebf9","modified":1481379406068},{"_id":"public/search.xml","hash":"2a7ab404ee1a53c685b2369b4c739c9b08380196","modified":1481379406068},{"_id":"public/about/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1481379406079},{"_id":"public/categories/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1481379406079},{"_id":"public/tags/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1481379406079},{"_id":"public/2016/12/09/17年要读的书和学习的技能/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1481379406079},{"_id":"public/2016/12/09/SparkDataFrameLikeSql/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1481379406079},{"_id":"public/2016/12/05/save-spark-rdd-into-Mysql/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1481379406079},{"_id":"public/2016/11/28/SparkStreamLearning/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1481379406080},{"_id":"public/2016/11/10/MAC访问你DOCKER容器中的WEB页面/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1481379406080},{"_id":"public/2016/11/09/Install-Caffe-on-CentOS/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1481379406080},{"_id":"public/2016/08/22/Spark-Window-Operation/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1481379406080},{"_id":"public/2016/08/19/Spark-2-0-Introduction/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1481379406080},{"_id":"public/2016/08/19/Save-DataFrame-into-a-partitioned-table-of-HIVE/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1481379406080},{"_id":"public/2016/08/11/SparkPassFunctions/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1481379406080},{"_id":"public/2016/07/26/MergeSort/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1481379406081},{"_id":"public/2016/07/26/ScalaList/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1481379406081},{"_id":"public/2016/07/19/Scala-call-by-name-call-by-value/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1481379406081},{"_id":"public/2016/07/13/机器学习相关材料/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1481379406081},{"_id":"public/2016/07/13/过拟合的原因/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1481379406081},{"_id":"public/2016/04/25/SPARK的宽依赖和窄依赖/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1481379406082},{"_id":"public/2016/04/21/1-100素数判断pyhon程序/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1481379406082},{"_id":"public/archives/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1481379406082},{"_id":"public/archives/page/2/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1481379406082},{"_id":"public/archives/2016/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1481379406082},{"_id":"public/archives/2016/page/2/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1481379406083},{"_id":"public/archives/2016/04/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1481379406083},{"_id":"public/archives/2016/07/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1481379406083},{"_id":"public/archives/2016/08/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1481379406083},{"_id":"public/archives/2016/11/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1481379406083},{"_id":"public/archives/2016/12/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1481379406083},{"_id":"public/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1481379406083},{"_id":"public/page/2/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1481379406084},{"_id":"public/tags/Python/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1481379406084},{"_id":"public/tags/Scala/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1481379406084},{"_id":"public/tags/Spark/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1481379406084},{"_id":"public/tags/Spark-Scala-Streaming/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1481379406085},{"_id":"public/tags/Spark-DataFrame/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1481379406090}],"Category":[],"Data":[],"Page":[{"title":"about","date":"2016-04-15T17:07:00.000Z","_content":"\n\nZhangShengShan, a machine learning hacker who believes that AI will create a better life for everybody.\n\n","source":"about/index.md","raw":"---\ntitle: about\ndate: 2016-04-16 01:07:00\n---\n\n\nZhangShengShan, a machine learning hacker who believes that AI will create a better life for everybody.\n\n","updated":"2016-12-10T14:11:24.624Z","path":"about/index.html","comments":1,"layout":"page","_id":"ciwjb1sqa0001dag36k6xza30","content":"<p>ZhangShengShan, a machine learning hacker who believes that AI will create a better life for everybody.</p>\n","excerpt":"","more":"<p>ZhangShengShan, a machine learning hacker who believes that AI will create a better life for everybody.</p>\n"},{"title":"categories","date":"2016-04-15T18:21:49.000Z","layout":"categories","_content":"","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2016-04-16 02:21:49\nlayout: categories\n---\n","updated":"2016-12-10T14:11:24.625Z","path":"categories/index.html","comments":1,"_id":"ciwjb1sss000ydag36qxal0lk","content":"","excerpt":"","more":""},{"title":"tags","layout":"tags","_content":"```\nhello world\n```\n\n\n{% blockquote %}\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque hendrerit lacus ut purus iaculis feugiat. Sed nec tempor elit, quis aliquam neque. Curabitur sed diam eget dolor fermentum semper at eu lorem.\n{% endblockquote %}\n","source":"tags/index.md","raw":"---\ntitle: tags\nlayout: tags\n---\n```\nhello world\n```\n\n\n{% blockquote %}\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque hendrerit lacus ut purus iaculis feugiat. Sed nec tempor elit, quis aliquam neque. Curabitur sed diam eget dolor fermentum semper at eu lorem.\n{% endblockquote %}\n","date":"2016-12-10T14:11:24.625Z","updated":"2016-12-10T14:11:24.625Z","path":"tags/index.html","comments":1,"_id":"ciwjb1sst000zdag3pohlk1s5","content":"<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hello world</span><br></pre></td></tr></table></figure>\n<blockquote><p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque hendrerit lacus ut purus iaculis feugiat. Sed nec tempor elit, quis aliquam neque. Curabitur sed diam eget dolor fermentum semper at eu lorem.</p>\n</blockquote>\n","excerpt":"","more":"<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hello world</span><br></pre></td></tr></table></figure>\n<blockquote><p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque hendrerit lacus ut purus iaculis feugiat. Sed nec tempor elit, quis aliquam neque. Curabitur sed diam eget dolor fermentum semper at eu lorem.</p>\n</blockquote>\n"}],"Post":[{"title":"1-100素数判断pyhon程序","date":"2016-04-21T10:08:04.000Z","_content":"\n\n求取1-100的所有素数，采用函数式编程\n```python\ndef issu(x):\n    result=map(lambda y:x%y,range(2,x))\n    if  len(result)!=0 and 0 not in result:\n        return 1\n    else:\n        return 0\n\n\nprint filter(issu,range(1,101))\n\n```\n答案为\n**[3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97]**\n","source":"_posts/1-100素数判断pyhon程序.md","raw":"---\ntitle: 1-100素数判断pyhon程序\ndate: 2016-04-21 18:08:04\ntags: Python\n---\n\n\n求取1-100的所有素数，采用函数式编程\n```python\ndef issu(x):\n    result=map(lambda y:x%y,range(2,x))\n    if  len(result)!=0 and 0 not in result:\n        return 1\n    else:\n        return 0\n\n\nprint filter(issu,range(1,101))\n\n```\n答案为\n**[3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97]**\n","slug":"1-100素数判断pyhon程序","published":1,"updated":"2016-12-10T14:11:24.624Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciwjb1sq10000dag3zsdlzbx3","content":"<p>求取1-100的所有素数，采用函数式编程<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">issu</span><span class=\"params\">(x)</span>:</span></span><br><span class=\"line\">    result=map(<span class=\"keyword\">lambda</span> y:x%y,range(<span class=\"number\">2</span>,x))</span><br><span class=\"line\">    <span class=\"keyword\">if</span>  len(result)!=<span class=\"number\">0</span> <span class=\"keyword\">and</span> <span class=\"number\">0</span> <span class=\"keyword\">not</span> <span class=\"keyword\">in</span> result:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">1</span></span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">print</span> filter(issu,range(<span class=\"number\">1</span>,<span class=\"number\">101</span>))</span><br></pre></td></tr></table></figure></p>\n<p>答案为<br><strong>[3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97]</strong></p>\n","excerpt":"","more":"<p>求取1-100的所有素数，采用函数式编程<br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">issu</span><span class=\"params\">(x)</span>:</span></span><br><span class=\"line\">    result=map(<span class=\"keyword\">lambda</span> y:x%y,range(<span class=\"number\">2</span>,x))</span><br><span class=\"line\">    <span class=\"keyword\">if</span>  len(result)!=<span class=\"number\">0</span> <span class=\"keyword\">and</span> <span class=\"number\">0</span> <span class=\"keyword\">not</span> <span class=\"keyword\">in</span> result:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">1</span></span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">print</span> filter(issu,range(<span class=\"number\">1</span>,<span class=\"number\">101</span>))</span><br></pre></td></tr></table></figure></p>\n<p>答案为<br><strong>[3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97]</strong></p>\n"},{"title":"17年要读的书和学习的技能","date":"2016-12-09T10:14:36.000Z","_content":"# 17年要读的书\n\n|书籍名称|目的        |\n|-------------|-------------|\n|算法艺术与信息学竞赛:算法竞赛入门经典(第2版) |提升算法水平，强化C／C++|\n|算法竞赛入门经典:训练指南 |提高算法，训练思维|\n|自己动手写Java虚拟机 |了解JAVA虚拟机原理，提升Golang编程水平|\n|响应式架构:消息模式Actor实现与Scala、Akka应用集成|学习并发编程、提升Scala编程水平|\n|[深度学习中文版](https://github.com/exacity/deeplearningbook-chinese)|学习深度学习，为该项目贡献代码|\n|分布式实时处理系统：原理、架构与实现 |深入理解分布式程序开发，提升C／C++实战能力|\n|深度学习：21天实战Caffe|实战深度学习，提升C／C++编程水平|\n\n# 17年要学习的技能\n机器学习方向\n-   DecisionTree\n-   XgBoost\n-   SVM \n-   AdaBoost\n-   PCA\n-   LogisticRegresion\n\nScala\n-   Akka\n-   Actor\n\nJAVA\n-   Spring SpringMVC MyBatis\n-   SpringBoot\n\nGolang\n-   Beego\n-   channel基础\n\n数学基础\n-   线性代数\n-   概率论\n\nSpark\n- Spark mlib\n- streaming(redis/kafka/akka/hbase)\n- structure streaming\n- basic rdd\n","source":"_posts/17年要读的书和学习的技能.md","raw":"---\ntitle: 17年要读的书和学习的技能\ndate: 2016-12-09 18:14:36\ntags:\n---\n# 17年要读的书\n\n|书籍名称|目的        |\n|-------------|-------------|\n|算法艺术与信息学竞赛:算法竞赛入门经典(第2版) |提升算法水平，强化C／C++|\n|算法竞赛入门经典:训练指南 |提高算法，训练思维|\n|自己动手写Java虚拟机 |了解JAVA虚拟机原理，提升Golang编程水平|\n|响应式架构:消息模式Actor实现与Scala、Akka应用集成|学习并发编程、提升Scala编程水平|\n|[深度学习中文版](https://github.com/exacity/deeplearningbook-chinese)|学习深度学习，为该项目贡献代码|\n|分布式实时处理系统：原理、架构与实现 |深入理解分布式程序开发，提升C／C++实战能力|\n|深度学习：21天实战Caffe|实战深度学习，提升C／C++编程水平|\n\n# 17年要学习的技能\n机器学习方向\n-   DecisionTree\n-   XgBoost\n-   SVM \n-   AdaBoost\n-   PCA\n-   LogisticRegresion\n\nScala\n-   Akka\n-   Actor\n\nJAVA\n-   Spring SpringMVC MyBatis\n-   SpringBoot\n\nGolang\n-   Beego\n-   channel基础\n\n数学基础\n-   线性代数\n-   概率论\n\nSpark\n- Spark mlib\n- streaming(redis/kafka/akka/hbase)\n- structure streaming\n- basic rdd\n","slug":"17年要读的书和学习的技能","published":1,"updated":"2016-12-10T14:11:24.624Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciwjb1sqc0002dag3dfj44omv","content":"<h1 id=\"17年要读的书\"><a href=\"#17年要读的书\" class=\"headerlink\" title=\"17年要读的书\"></a>17年要读的书</h1><table>\n<thead>\n<tr>\n<th>书籍名称</th>\n<th>目的</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>算法艺术与信息学竞赛:算法竞赛入门经典(第2版)</td>\n<td>提升算法水平，强化C／C++</td>\n</tr>\n<tr>\n<td>算法竞赛入门经典:训练指南</td>\n<td>提高算法，训练思维</td>\n</tr>\n<tr>\n<td>自己动手写Java虚拟机</td>\n<td>了解JAVA虚拟机原理，提升Golang编程水平</td>\n</tr>\n<tr>\n<td>响应式架构:消息模式Actor实现与Scala、Akka应用集成</td>\n<td>学习并发编程、提升Scala编程水平</td>\n</tr>\n<tr>\n<td><a href=\"https://github.com/exacity/deeplearningbook-chinese\" target=\"_blank\" rel=\"external\">深度学习中文版</a></td>\n<td>学习深度学习，为该项目贡献代码</td>\n</tr>\n<tr>\n<td>分布式实时处理系统：原理、架构与实现</td>\n<td>深入理解分布式程序开发，提升C／C++实战能力</td>\n</tr>\n<tr>\n<td>深度学习：21天实战Caffe</td>\n<td>实战深度学习，提升C／C++编程水平</td>\n</tr>\n</tbody>\n</table>\n<h1 id=\"17年要学习的技能\"><a href=\"#17年要学习的技能\" class=\"headerlink\" title=\"17年要学习的技能\"></a>17年要学习的技能</h1><p>机器学习方向</p>\n<ul>\n<li>DecisionTree</li>\n<li>XgBoost</li>\n<li>SVM </li>\n<li>AdaBoost</li>\n<li>PCA</li>\n<li>LogisticRegresion</li>\n</ul>\n<p>Scala</p>\n<ul>\n<li>Akka</li>\n<li>Actor</li>\n</ul>\n<p>JAVA</p>\n<ul>\n<li>Spring SpringMVC MyBatis</li>\n<li>SpringBoot</li>\n</ul>\n<p>Golang</p>\n<ul>\n<li>Beego</li>\n<li>channel基础</li>\n</ul>\n<p>数学基础</p>\n<ul>\n<li>线性代数</li>\n<li>概率论</li>\n</ul>\n<p>Spark</p>\n<ul>\n<li>Spark mlib</li>\n<li>streaming(redis/kafka/akka/hbase)</li>\n<li>structure streaming</li>\n<li>basic rdd</li>\n</ul>\n","excerpt":"","more":"<h1 id=\"17年要读的书\"><a href=\"#17年要读的书\" class=\"headerlink\" title=\"17年要读的书\"></a>17年要读的书</h1><table>\n<thead>\n<tr>\n<th>书籍名称</th>\n<th>目的</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>算法艺术与信息学竞赛:算法竞赛入门经典(第2版)</td>\n<td>提升算法水平，强化C／C++</td>\n</tr>\n<tr>\n<td>算法竞赛入门经典:训练指南</td>\n<td>提高算法，训练思维</td>\n</tr>\n<tr>\n<td>自己动手写Java虚拟机</td>\n<td>了解JAVA虚拟机原理，提升Golang编程水平</td>\n</tr>\n<tr>\n<td>响应式架构:消息模式Actor实现与Scala、Akka应用集成</td>\n<td>学习并发编程、提升Scala编程水平</td>\n</tr>\n<tr>\n<td><a href=\"https://github.com/exacity/deeplearningbook-chinese\">深度学习中文版</a></td>\n<td>学习深度学习，为该项目贡献代码</td>\n</tr>\n<tr>\n<td>分布式实时处理系统：原理、架构与实现</td>\n<td>深入理解分布式程序开发，提升C／C++实战能力</td>\n</tr>\n<tr>\n<td>深度学习：21天实战Caffe</td>\n<td>实战深度学习，提升C／C++编程水平</td>\n</tr>\n</tbody>\n</table>\n<h1 id=\"17年要学习的技能\"><a href=\"#17年要学习的技能\" class=\"headerlink\" title=\"17年要学习的技能\"></a>17年要学习的技能</h1><p>机器学习方向</p>\n<ul>\n<li>DecisionTree</li>\n<li>XgBoost</li>\n<li>SVM </li>\n<li>AdaBoost</li>\n<li>PCA</li>\n<li>LogisticRegresion</li>\n</ul>\n<p>Scala</p>\n<ul>\n<li>Akka</li>\n<li>Actor</li>\n</ul>\n<p>JAVA</p>\n<ul>\n<li>Spring SpringMVC MyBatis</li>\n<li>SpringBoot</li>\n</ul>\n<p>Golang</p>\n<ul>\n<li>Beego</li>\n<li>channel基础</li>\n</ul>\n<p>数学基础</p>\n<ul>\n<li>线性代数</li>\n<li>概率论</li>\n</ul>\n<p>Spark</p>\n<ul>\n<li>Spark mlib</li>\n<li>streaming(redis/kafka/akka/hbase)</li>\n<li>structure streaming</li>\n<li>basic rdd</li>\n</ul>\n"},{"title":"Install Caffe on CentOS","date":"2016-11-09T08:07:49.000Z","_content":"尽量不要在CentOS平台上安装Caffe\n","source":"_posts/Install-Caffe-on-CentOS.md","raw":"---\ntitle: Install Caffe on CentOS\ndate: 2016-11-09 16:07:49\ntags:\n---\n尽量不要在CentOS平台上安装Caffe\n","slug":"Install-Caffe-on-CentOS","published":1,"updated":"2016-12-10T14:11:24.624Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciwjb1sqn0004dag3x3gcky3z","content":"<p>尽量不要在CentOS平台上安装Caffe</p>\n","excerpt":"","more":"<p>尽量不要在CentOS平台上安装Caffe</p>\n"},{"title":"MergeSort","date":"2016-07-26T12:30:03.000Z","_content":"归并排序\n\n```\ndef msort[A](less: (A, A) => Boolean)(xs: List[A]): List[A] = { \n    def merge(xs1: List[A], xs2: List[A]): List[A] =\n        if (xs1.isEmpty) xs2\n        else if (xs2.isEmpty) xs1\n        else if (less(xs1.head, xs2.head)) xs1.head :: merge(xs1.tail, xs2) else xs2.head :: merge(xs1, xs2.tail)\n    val n = xs.length/2\n    if (n == 0) xs\n    else merge(msort(less)(xs take n), msort(less)(xs drop n))\n}\n```\n\n如果你对python列表的用法比较熟悉的话，可以按照如下的方式理解\n\n```\n    xs take n   // xs[0:n+1]\n    xs drop n   // xs[n+1:]\n```\n\nmsort函数应该按照如下方式进行调用\n```\nmsort((x: Int, y: Int) => x < y)(List(5, 7, 1, 3))\n```\n","source":"_posts/MergeSort.md","raw":"---\ntitle: MergeSort\ndate: 2016-07-26 20:30:03\ntags: Scala\n---\n归并排序\n\n```\ndef msort[A](less: (A, A) => Boolean)(xs: List[A]): List[A] = { \n    def merge(xs1: List[A], xs2: List[A]): List[A] =\n        if (xs1.isEmpty) xs2\n        else if (xs2.isEmpty) xs1\n        else if (less(xs1.head, xs2.head)) xs1.head :: merge(xs1.tail, xs2) else xs2.head :: merge(xs1, xs2.tail)\n    val n = xs.length/2\n    if (n == 0) xs\n    else merge(msort(less)(xs take n), msort(less)(xs drop n))\n}\n```\n\n如果你对python列表的用法比较熟悉的话，可以按照如下的方式理解\n\n```\n    xs take n   // xs[0:n+1]\n    xs drop n   // xs[n+1:]\n```\n\nmsort函数应该按照如下方式进行调用\n```\nmsort((x: Int, y: Int) => x < y)(List(5, 7, 1, 3))\n```\n","slug":"MergeSort","published":1,"updated":"2016-12-10T14:11:24.624Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciwjb1sqs0005dag390421a88","content":"<p>归并排序</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def msort[A](less: (A, A) =&gt; Boolean)(xs: List[A]): List[A] = &#123; </span><br><span class=\"line\">    def merge(xs1: List[A], xs2: List[A]): List[A] =</span><br><span class=\"line\">        if (xs1.isEmpty) xs2</span><br><span class=\"line\">        else if (xs2.isEmpty) xs1</span><br><span class=\"line\">        else if (less(xs1.head, xs2.head)) xs1.head :: merge(xs1.tail, xs2) else xs2.head :: merge(xs1, xs2.tail)</span><br><span class=\"line\">    val n = xs.length/2</span><br><span class=\"line\">    if (n == 0) xs</span><br><span class=\"line\">    else merge(msort(less)(xs take n), msort(less)(xs drop n))</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>如果你对python列表的用法比较熟悉的话，可以按照如下的方式理解</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">xs take n   // xs[0:n+1]</span><br><span class=\"line\">xs drop n   // xs[n+1:]</span><br></pre></td></tr></table></figure>\n<p>msort函数应该按照如下方式进行调用<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">msort((x: Int, y: Int) =&gt; x &lt; y)(List(5, 7, 1, 3))</span><br></pre></td></tr></table></figure></p>\n","excerpt":"","more":"<p>归并排序</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def msort[A](less: (A, A) =&gt; Boolean)(xs: List[A]): List[A] = &#123; </span><br><span class=\"line\">    def merge(xs1: List[A], xs2: List[A]): List[A] =</span><br><span class=\"line\">        if (xs1.isEmpty) xs2</span><br><span class=\"line\">        else if (xs2.isEmpty) xs1</span><br><span class=\"line\">        else if (less(xs1.head, xs2.head)) xs1.head :: merge(xs1.tail, xs2) else xs2.head :: merge(xs1, xs2.tail)</span><br><span class=\"line\">    val n = xs.length/2</span><br><span class=\"line\">    if (n == 0) xs</span><br><span class=\"line\">    else merge(msort(less)(xs take n), msort(less)(xs drop n))</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>如果你对python列表的用法比较熟悉的话，可以按照如下的方式理解</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">xs take n   // xs[0:n+1]</span><br><span class=\"line\">xs drop n   // xs[n+1:]</span><br></pre></td></tr></table></figure>\n<p>msort函数应该按照如下方式进行调用<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">msort((x: Int, y: Int) =&gt; x &lt; y)(List(5, 7, 1, 3))</span><br></pre></td></tr></table></figure></p>\n"},{"title":"MAC访问你DOCKER容器中的WEB页面","date":"2016-11-10T15:22:16.000Z","_content":"\n\n\ndocker run -d -p hostport:dockerport --name your_container_name  your_image_name nginx -g \"daemon off;\"\n\n\nthe above instruction start a docker nginx application which bind is port dockerport to its host port hostport.\nusually you can access the nginx service on your host environment by curl the hostport, however in MacOs, ths hostport here \nis the virtual machine. so when you curl localhost:hostport, you will get no response.\n\n\nthe right way is access the virtual machine responding port. so the ip of virtual machine is needed.\n\n```\n    docker-machine ip your_virtual_machine \n```\n\n\n```\n    curl the ip you get:hostport\n```\n\n\n\n\n\n\n","source":"_posts/MAC访问你DOCKER容器中的WEB页面.md","raw":"---\ntitle: MAC访问你DOCKER容器中的WEB页面\ndate: 2016-11-10 23:22:16\ntags:\n---\n\n\n\ndocker run -d -p hostport:dockerport --name your_container_name  your_image_name nginx -g \"daemon off;\"\n\n\nthe above instruction start a docker nginx application which bind is port dockerport to its host port hostport.\nusually you can access the nginx service on your host environment by curl the hostport, however in MacOs, ths hostport here \nis the virtual machine. so when you curl localhost:hostport, you will get no response.\n\n\nthe right way is access the virtual machine responding port. so the ip of virtual machine is needed.\n\n```\n    docker-machine ip your_virtual_machine \n```\n\n\n```\n    curl the ip you get:hostport\n```\n\n\n\n\n\n\n","slug":"MAC访问你DOCKER容器中的WEB页面","published":1,"updated":"2016-12-10T14:11:24.624Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciwjb1sqv0006dag30j7l5b2c","content":"<p>docker run -d -p hostport:dockerport –name your_container_name  your_image_name nginx -g “daemon off;”</p>\n<p>the above instruction start a docker nginx application which bind is port dockerport to its host port hostport.<br>usually you can access the nginx service on your host environment by curl the hostport, however in MacOs, ths hostport here<br>is the virtual machine. so when you curl localhost:hostport, you will get no response.</p>\n<p>the right way is access the virtual machine responding port. so the ip of virtual machine is needed.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker-machine ip your_virtual_machine</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl the ip you get:hostport</span><br></pre></td></tr></table></figure>\n","excerpt":"","more":"<p>docker run -d -p hostport:dockerport –name your_container_name  your_image_name nginx -g “daemon off;”</p>\n<p>the above instruction start a docker nginx application which bind is port dockerport to its host port hostport.<br>usually you can access the nginx service on your host environment by curl the hostport, however in MacOs, ths hostport here<br>is the virtual machine. so when you curl localhost:hostport, you will get no response.</p>\n<p>the right way is access the virtual machine responding port. so the ip of virtual machine is needed.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker-machine ip your_virtual_machine</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl the ip you get:hostport</span><br></pre></td></tr></table></figure>\n"},{"title":"SPARK的宽依赖和窄依赖","date":"2016-04-25T15:37:04.000Z","_content":"\n\nspark 的各种不同的transformation操作,可以根据是否依赖父RDDs的所有partision分为‘窄依赖’和‘宽依赖’,简单的说,有shuffle操作的就是宽依赖,而没有shuffle操作的就是窄依赖。\n对于窄依赖,spark会尽量将他们划分为同一个stage,而宽依赖则会称为另外的stage。\n\n","source":"_posts/SPARK的宽依赖和窄依赖.md","raw":"---\ntitle: SPARK的宽依赖和窄依赖\ndate: 2016-04-25 23:37:04\ntags: Spark\n---\n\n\nspark 的各种不同的transformation操作,可以根据是否依赖父RDDs的所有partision分为‘窄依赖’和‘宽依赖’,简单的说,有shuffle操作的就是宽依赖,而没有shuffle操作的就是窄依赖。\n对于窄依赖,spark会尽量将他们划分为同一个stage,而宽依赖则会称为另外的stage。\n\n","slug":"SPARK的宽依赖和窄依赖","published":1,"updated":"2016-12-10T14:11:24.624Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciwjb1sqz0009dag377iugm43","content":"<p>spark 的各种不同的transformation操作,可以根据是否依赖父RDDs的所有partision分为‘窄依赖’和‘宽依赖’,简单的说,有shuffle操作的就是宽依赖,而没有shuffle操作的就是窄依赖。<br>对于窄依赖,spark会尽量将他们划分为同一个stage,而宽依赖则会称为另外的stage。</p>\n","excerpt":"","more":"<p>spark 的各种不同的transformation操作,可以根据是否依赖父RDDs的所有partision分为‘窄依赖’和‘宽依赖’,简单的说,有shuffle操作的就是宽依赖,而没有shuffle操作的就是窄依赖。<br>对于窄依赖,spark会尽量将他们划分为同一个stage,而宽依赖则会称为另外的stage。</p>\n"},{"title":"Save DataFrame into a partitioned table of HIVE","date":"2016-08-19T09:15:15.000Z","_content":"\n\n# How to save a spark DataFrame as a patitioned hive table #\n## utilise saveAsTable method ##\n\n```\n    val conf = new SparkConf().setAppName(\"Simple Application\").setMaster(\"local\")\n    val sc = new SparkContext(conf)\n    val sqlContext = new org.apache.spark.sql.SQLContext(sc)\n    import sqlContext.implicits._\n    val hiveContext = new org.apache.spark.sql.hive.HiveContext(sc)\n    hiveContext.sql(\"use database\")\n\n    val cmd =\n      \"\"\"\n         select\n          col1,\n          col2\n         from\n          table\n      \"\"\".stripMargin\n    val yourDf = hiveContext.sql(cmd)\n    yourDf.printSchema()\n    yourDf.write.partitionBy(\"col2\").saveAsTable(\"partitionTableName\")\n```\n\n\n\n\n","source":"_posts/Save-DataFrame-into-a-partitioned-table-of-HIVE.md","raw":"---\ntitle: Save DataFrame into a partitioned table of HIVE\ndate: 2016-08-19 17:15:15\ntags: Spark\n---\n\n\n# How to save a spark DataFrame as a patitioned hive table #\n## utilise saveAsTable method ##\n\n```\n    val conf = new SparkConf().setAppName(\"Simple Application\").setMaster(\"local\")\n    val sc = new SparkContext(conf)\n    val sqlContext = new org.apache.spark.sql.SQLContext(sc)\n    import sqlContext.implicits._\n    val hiveContext = new org.apache.spark.sql.hive.HiveContext(sc)\n    hiveContext.sql(\"use database\")\n\n    val cmd =\n      \"\"\"\n         select\n          col1,\n          col2\n         from\n          table\n      \"\"\".stripMargin\n    val yourDf = hiveContext.sql(cmd)\n    yourDf.printSchema()\n    yourDf.write.partitionBy(\"col2\").saveAsTable(\"partitionTableName\")\n```\n\n\n\n\n","slug":"Save-DataFrame-into-a-partitioned-table-of-HIVE","published":1,"updated":"2016-12-10T14:11:24.624Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciwjb1sr8000adag3xr0h92ji","content":"<h1 id=\"How-to-save-a-spark-DataFrame-as-a-patitioned-hive-table\"><a href=\"#How-to-save-a-spark-DataFrame-as-a-patitioned-hive-table\" class=\"headerlink\" title=\"How to save a spark DataFrame as a patitioned hive table\"></a>How to save a spark DataFrame as a patitioned hive table</h1><h2 id=\"utilise-saveAsTable-method\"><a href=\"#utilise-saveAsTable-method\" class=\"headerlink\" title=\"utilise saveAsTable method\"></a>utilise saveAsTable method</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">val conf = new SparkConf().setAppName(&quot;Simple Application&quot;).setMaster(&quot;local&quot;)</span><br><span class=\"line\">val sc = new SparkContext(conf)</span><br><span class=\"line\">val sqlContext = new org.apache.spark.sql.SQLContext(sc)</span><br><span class=\"line\">import sqlContext.implicits._</span><br><span class=\"line\">val hiveContext = new org.apache.spark.sql.hive.HiveContext(sc)</span><br><span class=\"line\">hiveContext.sql(&quot;use database&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">val cmd =</span><br><span class=\"line\">  &quot;&quot;&quot;</span><br><span class=\"line\">     select</span><br><span class=\"line\">      col1,</span><br><span class=\"line\">      col2</span><br><span class=\"line\">     from</span><br><span class=\"line\">      table</span><br><span class=\"line\">  &quot;&quot;&quot;.stripMargin</span><br><span class=\"line\">val yourDf = hiveContext.sql(cmd)</span><br><span class=\"line\">yourDf.printSchema()</span><br><span class=\"line\">yourDf.write.partitionBy(&quot;col2&quot;).saveAsTable(&quot;partitionTableName&quot;)</span><br></pre></td></tr></table></figure>\n","excerpt":"","more":"<h1 id=\"How-to-save-a-spark-DataFrame-as-a-patitioned-hive-table\"><a href=\"#How-to-save-a-spark-DataFrame-as-a-patitioned-hive-table\" class=\"headerlink\" title=\"How to save a spark DataFrame as a patitioned hive table\"></a>How to save a spark DataFrame as a patitioned hive table</h1><h2 id=\"utilise-saveAsTable-method\"><a href=\"#utilise-saveAsTable-method\" class=\"headerlink\" title=\"utilise saveAsTable method\"></a>utilise saveAsTable method</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">val conf = new SparkConf().setAppName(&quot;Simple Application&quot;).setMaster(&quot;local&quot;)</span><br><span class=\"line\">val sc = new SparkContext(conf)</span><br><span class=\"line\">val sqlContext = new org.apache.spark.sql.SQLContext(sc)</span><br><span class=\"line\">import sqlContext.implicits._</span><br><span class=\"line\">val hiveContext = new org.apache.spark.sql.hive.HiveContext(sc)</span><br><span class=\"line\">hiveContext.sql(&quot;use database&quot;)</span><br><span class=\"line\"></span><br><span class=\"line\">val cmd =</span><br><span class=\"line\">  &quot;&quot;&quot;</span><br><span class=\"line\">     select</span><br><span class=\"line\">      col1,</span><br><span class=\"line\">      col2</span><br><span class=\"line\">     from</span><br><span class=\"line\">      table</span><br><span class=\"line\">  &quot;&quot;&quot;.stripMargin</span><br><span class=\"line\">val yourDf = hiveContext.sql(cmd)</span><br><span class=\"line\">yourDf.printSchema()</span><br><span class=\"line\">yourDf.write.partitionBy(&quot;col2&quot;).saveAsTable(&quot;partitionTableName&quot;)</span><br></pre></td></tr></table></figure>\n"},{"title":"Scala call-by-name call-by-value","date":"2016-07-19T15:30:34.000Z","_content":"Call-by-value has the advantage that it avoids repeated evaluation of arguments.\nCall-by-name has the advantage that it avoids evaluation of arguments when the\nparameter is not used at all by the function. Call-by-value is usually more efficient\nthan call-by-name, but a call-by-value evaluation might loop where a call-by-name\nevaluation would terminate. Consider:\n\nCall-by-value 的优势在于避免不断的计算参数。而call-by-name的优势在于如果一个函数根本就不会用到的参数，那么也不会被计算，与call-by-value恰好相反。下面的例子展示了一个Call-by-value会不停循环但是Call-by-name会停止的例子。\n\n```\nscala> def loop: Int = loop\nloop: Int\nscala> def first(x: Int, y: Int) = x\nfirst: (Int,Int)Int\n```\nThen first(1, loop) reduces with call-by-name to 1, whereas the same term reduces with call-by-value repeatedly to itself, hence evaluation does not terminate.\nfirst(1, loop)\n→ first(1, loop)\n→ first(1, loop)\n→ ...\n上面的例子，之所以不停的循环的原因就是,y 被声明为 Call-by-value，因而，按照上面的说法，无论是否这个参数会被用到，该参数都会被计算，所以会不停的循环。\n\nScala uses call-by-value by default, but it switches to call-by-name evaluation if the\nparameter type is preceded by =>.\n```\nscala> def constOne(x: Int, y: => Int) = 1\nconstOne: (Int,=> Int)Int\nscala> constOne(1, loop)\nunnamed0: Int = 1\nscala> constOne(loop, 2) // gives an infinite loop.\n```\n\n\nconstOne(1,loop) 会停止，y被声明为Call-by-name, 所以当没有用到这个参数的时候,则不会被计算，因此不会陷入无限循环。\nconstOne(loop,2) 则恰好相反。\n\n\n\n\n本文示例 来自于  《ScalaByExample》,感谢原作者。\n\n\n\n","source":"_posts/Scala-call-by-name-call-by-value.md","raw":"---\ntitle: Scala call-by-name call-by-value\ndate: 2016-07-19 23:30:34\ntags: Scala\n---\nCall-by-value has the advantage that it avoids repeated evaluation of arguments.\nCall-by-name has the advantage that it avoids evaluation of arguments when the\nparameter is not used at all by the function. Call-by-value is usually more efficient\nthan call-by-name, but a call-by-value evaluation might loop where a call-by-name\nevaluation would terminate. Consider:\n\nCall-by-value 的优势在于避免不断的计算参数。而call-by-name的优势在于如果一个函数根本就不会用到的参数，那么也不会被计算，与call-by-value恰好相反。下面的例子展示了一个Call-by-value会不停循环但是Call-by-name会停止的例子。\n\n```\nscala> def loop: Int = loop\nloop: Int\nscala> def first(x: Int, y: Int) = x\nfirst: (Int,Int)Int\n```\nThen first(1, loop) reduces with call-by-name to 1, whereas the same term reduces with call-by-value repeatedly to itself, hence evaluation does not terminate.\nfirst(1, loop)\n→ first(1, loop)\n→ first(1, loop)\n→ ...\n上面的例子，之所以不停的循环的原因就是,y 被声明为 Call-by-value，因而，按照上面的说法，无论是否这个参数会被用到，该参数都会被计算，所以会不停的循环。\n\nScala uses call-by-value by default, but it switches to call-by-name evaluation if the\nparameter type is preceded by =>.\n```\nscala> def constOne(x: Int, y: => Int) = 1\nconstOne: (Int,=> Int)Int\nscala> constOne(1, loop)\nunnamed0: Int = 1\nscala> constOne(loop, 2) // gives an infinite loop.\n```\n\n\nconstOne(1,loop) 会停止，y被声明为Call-by-name, 所以当没有用到这个参数的时候,则不会被计算，因此不会陷入无限循环。\nconstOne(loop,2) 则恰好相反。\n\n\n\n\n本文示例 来自于  《ScalaByExample》,感谢原作者。\n\n\n\n","slug":"Scala-call-by-name-call-by-value","published":1,"updated":"2016-12-10T14:11:24.624Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciwjb1srb000cdag3dcz8n49e","content":"<p>Call-by-value has the advantage that it avoids repeated evaluation of arguments.<br>Call-by-name has the advantage that it avoids evaluation of arguments when the<br>parameter is not used at all by the function. Call-by-value is usually more efficient<br>than call-by-name, but a call-by-value evaluation might loop where a call-by-name<br>evaluation would terminate. Consider:</p>\n<p>Call-by-value 的优势在于避免不断的计算参数。而call-by-name的优势在于如果一个函数根本就不会用到的参数，那么也不会被计算，与call-by-value恰好相反。下面的例子展示了一个Call-by-value会不停循环但是Call-by-name会停止的例子。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">scala&gt; def loop: Int = loop</span><br><span class=\"line\">loop: Int</span><br><span class=\"line\">scala&gt; def first(x: Int, y: Int) = x</span><br><span class=\"line\">first: (Int,Int)Int</span><br></pre></td></tr></table></figure>\n<p>Then first(1, loop) reduces with call-by-name to 1, whereas the same term reduces with call-by-value repeatedly to itself, hence evaluation does not terminate.<br>first(1, loop)<br>→ first(1, loop)<br>→ first(1, loop)<br>→ …<br>上面的例子，之所以不停的循环的原因就是,y 被声明为 Call-by-value，因而，按照上面的说法，无论是否这个参数会被用到，该参数都会被计算，所以会不停的循环。</p>\n<p>Scala uses call-by-value by default, but it switches to call-by-name evaluation if the<br>parameter type is preceded by =&gt;.<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">scala&gt; def constOne(x: Int, y: =&gt; Int) = 1</span><br><span class=\"line\">constOne: (Int,=&gt; Int)Int</span><br><span class=\"line\">scala&gt; constOne(1, loop)</span><br><span class=\"line\">unnamed0: Int = 1</span><br><span class=\"line\">scala&gt; constOne(loop, 2) // gives an infinite loop.</span><br></pre></td></tr></table></figure></p>\n<p>constOne(1,loop) 会停止，y被声明为Call-by-name, 所以当没有用到这个参数的时候,则不会被计算，因此不会陷入无限循环。<br>constOne(loop,2) 则恰好相反。</p>\n<p>本文示例 来自于  《ScalaByExample》,感谢原作者。</p>\n","excerpt":"","more":"<p>Call-by-value has the advantage that it avoids repeated evaluation of arguments.<br>Call-by-name has the advantage that it avoids evaluation of arguments when the<br>parameter is not used at all by the function. Call-by-value is usually more efficient<br>than call-by-name, but a call-by-value evaluation might loop where a call-by-name<br>evaluation would terminate. Consider:</p>\n<p>Call-by-value 的优势在于避免不断的计算参数。而call-by-name的优势在于如果一个函数根本就不会用到的参数，那么也不会被计算，与call-by-value恰好相反。下面的例子展示了一个Call-by-value会不停循环但是Call-by-name会停止的例子。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">scala&gt; def loop: Int = loop</span><br><span class=\"line\">loop: Int</span><br><span class=\"line\">scala&gt; def first(x: Int, y: Int) = x</span><br><span class=\"line\">first: (Int,Int)Int</span><br></pre></td></tr></table></figure>\n<p>Then first(1, loop) reduces with call-by-name to 1, whereas the same term reduces with call-by-value repeatedly to itself, hence evaluation does not terminate.<br>first(1, loop)<br>→ first(1, loop)<br>→ first(1, loop)<br>→ …<br>上面的例子，之所以不停的循环的原因就是,y 被声明为 Call-by-value，因而，按照上面的说法，无论是否这个参数会被用到，该参数都会被计算，所以会不停的循环。</p>\n<p>Scala uses call-by-value by default, but it switches to call-by-name evaluation if the<br>parameter type is preceded by =&gt;.<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">scala&gt; def constOne(x: Int, y: =&gt; Int) = 1</span><br><span class=\"line\">constOne: (Int,=&gt; Int)Int</span><br><span class=\"line\">scala&gt; constOne(1, loop)</span><br><span class=\"line\">unnamed0: Int = 1</span><br><span class=\"line\">scala&gt; constOne(loop, 2) // gives an infinite loop.</span><br></pre></td></tr></table></figure></p>\n<p>constOne(1,loop) 会停止，y被声明为Call-by-name, 所以当没有用到这个参数的时候,则不会被计算，因此不会陷入无限循环。<br>constOne(loop,2) 则恰好相反。</p>\n<p>本文示例 来自于  《ScalaByExample》,感谢原作者。</p>\n"},{"title":"ScalaList","date":"2016-07-26T05:11:12.000Z","_content":"Lists are not built in in Scala; they are defined by an abstract class List, **which comes with two subclasses for :: and Nil.**\nList 并不是Scala的内置类型。List被定义为抽象类。\n\n```\n    package scala\n    abstract class List[+A] {\n```\nList is an abstract class, so one cannot define elements by calling the empty List constructor (e.g. by new List). The class has a type parameter a. It is co-variant in this parameter,which means thatList[S] <: List[T] for all types S and T such thatS <: T.The class is situated in the package scala.This is a package containing the most important standard classes of Scala. List defines a number of methods, which are explained in the following.\nList 是抽象类，所以没有办法通过空的List构造器来定义元素。List存在一个类型参数A。该参数是协变类型, 对于任意类型S和T，如果S<:T, 则 List[S]<:List[T]。该类的定义在scala package中。这个包是Scala中最重要的标准calsses。\n","source":"_posts/ScalaList.md","raw":"---\ntitle: ScalaList\ndate: 2016-07-26 13:11:12\ntags: Scala\n---\nLists are not built in in Scala; they are defined by an abstract class List, **which comes with two subclasses for :: and Nil.**\nList 并不是Scala的内置类型。List被定义为抽象类。\n\n```\n    package scala\n    abstract class List[+A] {\n```\nList is an abstract class, so one cannot define elements by calling the empty List constructor (e.g. by new List). The class has a type parameter a. It is co-variant in this parameter,which means thatList[S] <: List[T] for all types S and T such thatS <: T.The class is situated in the package scala.This is a package containing the most important standard classes of Scala. List defines a number of methods, which are explained in the following.\nList 是抽象类，所以没有办法通过空的List构造器来定义元素。List存在一个类型参数A。该参数是协变类型, 对于任意类型S和T，如果S<:T, 则 List[S]<:List[T]。该类的定义在scala package中。这个包是Scala中最重要的标准calsses。\n","slug":"ScalaList","published":1,"updated":"2016-12-10T14:11:24.624Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciwjb1sri000edag39hompxi7","content":"<p>Lists are not built in in Scala; they are defined by an abstract class List, <strong>which comes with two subclasses for :: and Nil.</strong><br>List 并不是Scala的内置类型。List被定义为抽象类。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">package scala</span><br><span class=\"line\">abstract class List[+A] &#123;</span><br></pre></td></tr></table></figure>\n<p>List is an abstract class, so one cannot define elements by calling the empty List constructor (e.g. by new List). The class has a type parameter a. It is co-variant in this parameter,which means thatList[S] &lt;: List[T] for all types S and T such thatS &lt;: T.The class is situated in the package scala.This is a package containing the most important standard classes of Scala. List defines a number of methods, which are explained in the following.<br>List 是抽象类，所以没有办法通过空的List构造器来定义元素。List存在一个类型参数A。该参数是协变类型, 对于任意类型S和T，如果S&lt;:T, 则 List[S]&lt;:List[T]。该类的定义在scala package中。这个包是Scala中最重要的标准calsses。</p>\n","excerpt":"","more":"<p>Lists are not built in in Scala; they are defined by an abstract class List, <strong>which comes with two subclasses for :: and Nil.</strong><br>List 并不是Scala的内置类型。List被定义为抽象类。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">package scala</span><br><span class=\"line\">abstract class List[+A] &#123;</span><br></pre></td></tr></table></figure>\n<p>List is an abstract class, so one cannot define elements by calling the empty List constructor (e.g. by new List). The class has a type parameter a. It is co-variant in this parameter,which means thatList[S] &lt;: List[T] for all types S and T such thatS &lt;: T.The class is situated in the package scala.This is a package containing the most important standard classes of Scala. List defines a number of methods, which are explained in the following.<br>List 是抽象类，所以没有办法通过空的List构造器来定义元素。List存在一个类型参数A。该参数是协变类型, 对于任意类型S和T，如果S&lt;:T, 则 List[S]&lt;:List[T]。该类的定义在scala package中。这个包是Scala中最重要的标准calsses。</p>\n"},{"title":"Spark 2.0 Introduction","date":"2016-08-19T10:04:07.000Z","_content":"\n# Spark 2.0 MLib Introduction #\n\n\nAs of Spark 2.0, the RDD-based APIs in the spark.mllib package have entered maintenance mode. The primary Machine Learning API for Spark is now the DataFrame-based API in the spark.ml package.\n\nSpark2.0 ,在spark.mllib中的基于RDD的机器学习APIs将会进入维护模式。现在机器学习的主要的API基于DataFrame,位于spark.ml中。\n\n\n\nWhat are the implications?\n\n    MLlib will still support the RDD-based API in spark.mllib with bug fixes.\n    MLlib will not add new features to the RDD-based API.\n    In the Spark 2.x releases, MLlib will add features to the DataFrames-based API to reach feature parity with the RDD-based API.\n    After reaching feature parity (roughly estimated for Spark 2.2), the RDD-based API will be deprecated.\n    The RDD-based API is expected to be removed in Spark 3.0.\n\n\nWhy is MLlib switching to the DataFrame-based API?\n\n    DataFrames provide a more user-friendly API than RDDs. The many benefits of DataFrames include Spark Datasources, SQL/DataFrame queries, Tungsten and Catalyst optimizations, and uniform APIs across languages.\n    The DataFrame-based API for MLlib provides a uniform API across ML algorithms and across multiple languages.\n    DataFrames facilitate practical ML Pipelines, particularly feature transformations. See the Pipelines guide for details.\n\n\n\n\n","source":"_posts/Spark-2-0-Introduction.md","raw":"---\ntitle: Spark 2.0 Introduction\ndate: 2016-08-19 18:04:07\ntags: Spark\n---\n\n# Spark 2.0 MLib Introduction #\n\n\nAs of Spark 2.0, the RDD-based APIs in the spark.mllib package have entered maintenance mode. The primary Machine Learning API for Spark is now the DataFrame-based API in the spark.ml package.\n\nSpark2.0 ,在spark.mllib中的基于RDD的机器学习APIs将会进入维护模式。现在机器学习的主要的API基于DataFrame,位于spark.ml中。\n\n\n\nWhat are the implications?\n\n    MLlib will still support the RDD-based API in spark.mllib with bug fixes.\n    MLlib will not add new features to the RDD-based API.\n    In the Spark 2.x releases, MLlib will add features to the DataFrames-based API to reach feature parity with the RDD-based API.\n    After reaching feature parity (roughly estimated for Spark 2.2), the RDD-based API will be deprecated.\n    The RDD-based API is expected to be removed in Spark 3.0.\n\n\nWhy is MLlib switching to the DataFrame-based API?\n\n    DataFrames provide a more user-friendly API than RDDs. The many benefits of DataFrames include Spark Datasources, SQL/DataFrame queries, Tungsten and Catalyst optimizations, and uniform APIs across languages.\n    The DataFrame-based API for MLlib provides a uniform API across ML algorithms and across multiple languages.\n    DataFrames facilitate practical ML Pipelines, particularly feature transformations. See the Pipelines guide for details.\n\n\n\n\n","slug":"Spark-2-0-Introduction","published":1,"updated":"2016-12-10T14:11:24.624Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciwjb1srm000gdag3j1pyoj68","content":"<h1 id=\"Spark-2-0-MLib-Introduction\"><a href=\"#Spark-2-0-MLib-Introduction\" class=\"headerlink\" title=\"Spark 2.0 MLib Introduction\"></a>Spark 2.0 MLib Introduction</h1><p>As of Spark 2.0, the RDD-based APIs in the spark.mllib package have entered maintenance mode. The primary Machine Learning API for Spark is now the DataFrame-based API in the spark.ml package.</p>\n<p>Spark2.0 ,在spark.mllib中的基于RDD的机器学习APIs将会进入维护模式。现在机器学习的主要的API基于DataFrame,位于spark.ml中。</p>\n<p>What are the implications?</p>\n<pre><code>MLlib will still support the RDD-based API in spark.mllib with bug fixes.\nMLlib will not add new features to the RDD-based API.\nIn the Spark 2.x releases, MLlib will add features to the DataFrames-based API to reach feature parity with the RDD-based API.\nAfter reaching feature parity (roughly estimated for Spark 2.2), the RDD-based API will be deprecated.\nThe RDD-based API is expected to be removed in Spark 3.0.\n</code></pre><p>Why is MLlib switching to the DataFrame-based API?</p>\n<pre><code>DataFrames provide a more user-friendly API than RDDs. The many benefits of DataFrames include Spark Datasources, SQL/DataFrame queries, Tungsten and Catalyst optimizations, and uniform APIs across languages.\nThe DataFrame-based API for MLlib provides a uniform API across ML algorithms and across multiple languages.\nDataFrames facilitate practical ML Pipelines, particularly feature transformations. See the Pipelines guide for details.\n</code></pre>","excerpt":"","more":"<h1 id=\"Spark-2-0-MLib-Introduction\"><a href=\"#Spark-2-0-MLib-Introduction\" class=\"headerlink\" title=\"Spark 2.0 MLib Introduction\"></a>Spark 2.0 MLib Introduction</h1><p>As of Spark 2.0, the RDD-based APIs in the spark.mllib package have entered maintenance mode. The primary Machine Learning API for Spark is now the DataFrame-based API in the spark.ml package.</p>\n<p>Spark2.0 ,在spark.mllib中的基于RDD的机器学习APIs将会进入维护模式。现在机器学习的主要的API基于DataFrame,位于spark.ml中。</p>\n<p>What are the implications?</p>\n<pre><code>MLlib will still support the RDD-based API in spark.mllib with bug fixes.\nMLlib will not add new features to the RDD-based API.\nIn the Spark 2.x releases, MLlib will add features to the DataFrames-based API to reach feature parity with the RDD-based API.\nAfter reaching feature parity (roughly estimated for Spark 2.2), the RDD-based API will be deprecated.\nThe RDD-based API is expected to be removed in Spark 3.0.\n</code></pre><p>Why is MLlib switching to the DataFrame-based API?</p>\n<pre><code>DataFrames provide a more user-friendly API than RDDs. The many benefits of DataFrames include Spark Datasources, SQL/DataFrame queries, Tungsten and Catalyst optimizations, and uniform APIs across languages.\nThe DataFrame-based API for MLlib provides a uniform API across ML algorithms and across multiple languages.\nDataFrames facilitate practical ML Pipelines, particularly feature transformations. See the Pipelines guide for details.\n</code></pre>"},{"title":"Spark Window Operation ","date":"2016-08-22T06:39:58.000Z","_content":"","source":"_posts/Spark-Window-Operation.md","raw":"---\ntitle: 'Spark Window Operation '\ndate: 2016-08-22 14:39:58\ntags:\n---\n","slug":"Spark-Window-Operation","published":1,"updated":"2016-12-10T14:11:24.624Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciwjb1srr000jdag3tszhafzt","content":"","excerpt":"","more":""},{"title":"SparkDataFrameLikeSql","date":"2016-12-09T08:11:00.000Z","_content":"\nThe idea of spark Datafame is inspired from dataframe of pandas which is a package of python for computing. On my opinion, dataframe can by prefered by the people who is familiar with SQL or BI developers, for it is easy to learn.\n\n\nDataFrame could by registered as a table ,then Somebody could explore the the data by using Standard SQL.\n\nHowever this article will focus on  some dataframe processing method without the help of registering a virtual table, and compared to those common operations in SQL including  SELECT, WHERE, GROUPBY, MIN, MAX, COUNT, SUM ,DISTINCT, ORDERBY, TOP, JOIN and so on\n\n\n```\n    select * from person;\n\n\n```\n","source":"_posts/SparkDataFrameLikeSql.md","raw":"---\ntitle: SparkDataFrameLikeSql\ndate: 2016-12-09 16:11:00\ntags: Spark DataFrame\n---\n\nThe idea of spark Datafame is inspired from dataframe of pandas which is a package of python for computing. On my opinion, dataframe can by prefered by the people who is familiar with SQL or BI developers, for it is easy to learn.\n\n\nDataFrame could by registered as a table ,then Somebody could explore the the data by using Standard SQL.\n\nHowever this article will focus on  some dataframe processing method without the help of registering a virtual table, and compared to those common operations in SQL including  SELECT, WHERE, GROUPBY, MIN, MAX, COUNT, SUM ,DISTINCT, ORDERBY, TOP, JOIN and so on\n\n\n```\n    select * from person;\n\n\n```\n","slug":"SparkDataFrameLikeSql","published":1,"updated":"2016-12-10T14:16:36.104Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciwjb1sru000ldag3wy71j5q4","content":"<p>The idea of spark Datafame is inspired from dataframe of pandas which is a package of python for computing. On my opinion, dataframe can by prefered by the people who is familiar with SQL or BI developers, for it is easy to learn.</p>\n<p>DataFrame could by registered as a table ,then Somebody could explore the the data by using Standard SQL.</p>\n<p>However this article will focus on  some dataframe processing method without the help of registering a virtual table, and compared to those common operations in SQL including  SELECT, WHERE, GROUPBY, MIN, MAX, COUNT, SUM ,DISTINCT, ORDERBY, TOP, JOIN and so on</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">select * from person;</span><br></pre></td></tr></table></figure>\n","excerpt":"","more":"<p>The idea of spark Datafame is inspired from dataframe of pandas which is a package of python for computing. On my opinion, dataframe can by prefered by the people who is familiar with SQL or BI developers, for it is easy to learn.</p>\n<p>DataFrame could by registered as a table ,then Somebody could explore the the data by using Standard SQL.</p>\n<p>However this article will focus on  some dataframe processing method without the help of registering a virtual table, and compared to those common operations in SQL including  SELECT, WHERE, GROUPBY, MIN, MAX, COUNT, SUM ,DISTINCT, ORDERBY, TOP, JOIN and so on</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">select * from person;</span><br></pre></td></tr></table></figure>\n"},{"title":"SparkPassFunctions","date":"2016-08-11T07:19:37.000Z","_content":"\n\n\n```\n    class MyClass {\n      val field = \"Hello\"\n      def doStuff(rdd: RDD[String]): RDD[String] = {\n      val field_ = this.field\n      rdd.map(x => field_ + x)}\n    }\n```\n","source":"_posts/SparkPassFunctions.md","raw":"---\ntitle: SparkPassFunctions\ndate: 2016-08-11 15:19:37\ntags: Spark\n---\n\n\n\n```\n    class MyClass {\n      val field = \"Hello\"\n      def doStuff(rdd: RDD[String]): RDD[String] = {\n      val field_ = this.field\n      rdd.map(x => field_ + x)}\n    }\n```\n","slug":"SparkPassFunctions","published":1,"updated":"2016-12-10T14:11:24.624Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciwjb1sry000ndag3dgoitpez","content":"<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class MyClass &#123;</span><br><span class=\"line\">  val field = &quot;Hello&quot;</span><br><span class=\"line\">  def doStuff(rdd: RDD[String]): RDD[String] = &#123;</span><br><span class=\"line\">  val field_ = this.field</span><br><span class=\"line\">  rdd.map(x =&gt; field_ + x)&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n","excerpt":"","more":"<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class MyClass &#123;</span><br><span class=\"line\">  val field = &quot;Hello&quot;</span><br><span class=\"line\">  def doStuff(rdd: RDD[String]): RDD[String] = &#123;</span><br><span class=\"line\">  val field_ = this.field</span><br><span class=\"line\">  rdd.map(x =&gt; field_ + x)&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n"},{"title":"SparkStreamLearning","date":"2016-11-28T05:16:15.000Z","_content":"# input source \nkafka \nakka\n# output \nredis\nkafka\nelasticSearch\nhive\nmySql\n","source":"_posts/SparkStreamLearning.md","raw":"---\ntitle: SparkStreamLearning\ndate: 2016-11-28 13:16:15\ntags: Spark Scala Streaming\n---\n# input source \nkafka \nakka\n# output \nredis\nkafka\nelasticSearch\nhive\nmySql\n","slug":"SparkStreamLearning","published":1,"updated":"2016-12-10T14:11:24.624Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciwjb1ss1000qdag3ba1w7ijk","content":"<h1 id=\"input-source\"><a href=\"#input-source\" class=\"headerlink\" title=\"input source\"></a>input source</h1><p>kafka<br>akka</p>\n<h1 id=\"output\"><a href=\"#output\" class=\"headerlink\" title=\"output\"></a>output</h1><p>redis<br>kafka<br>elasticSearch<br>hive<br>mySql</p>\n","excerpt":"","more":"<h1 id=\"input-source\"><a href=\"#input-source\" class=\"headerlink\" title=\"input source\"></a>input source</h1><p>kafka<br>akka</p>\n<h1 id=\"output\"><a href=\"#output\" class=\"headerlink\" title=\"output\"></a>output</h1><p>redis<br>kafka<br>elasticSearch<br>hive<br>mySql</p>\n"},{"title":"save spark rdd into Mysql","date":"2016-12-05T14:12:19.000Z","_content":"\n```\nimport java.util.Properties\nval target_df = targetRdd.toDF()\nval prop = new Properties()\nprop.put(\"user\", \"username\")\nprop.put(\"password\", \"password\")\nret_df.write.mode(\"append\").jdbc(\"jdbc:mysql://host:port/database\",\"table\",prop)\n```\n\n","source":"_posts/save-spark-rdd-into-Mysql.md","raw":"---\ntitle: save spark rdd into Mysql\ndate: 2016-12-05 22:12:19\ntags:\n---\n\n```\nimport java.util.Properties\nval target_df = targetRdd.toDF()\nval prop = new Properties()\nprop.put(\"user\", \"username\")\nprop.put(\"password\", \"password\")\nret_df.write.mode(\"append\").jdbc(\"jdbc:mysql://host:port/database\",\"table\",prop)\n```\n\n","slug":"save-spark-rdd-into-Mysql","published":1,"updated":"2016-12-10T14:11:24.624Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciwjb1ss4000sdag3u4wr9c25","content":"<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import java.util.Properties</span><br><span class=\"line\">val target_df = targetRdd.toDF()</span><br><span class=\"line\">val prop = new Properties()</span><br><span class=\"line\">prop.put(&quot;user&quot;, &quot;username&quot;)</span><br><span class=\"line\">prop.put(&quot;password&quot;, &quot;password&quot;)</span><br><span class=\"line\">ret_df.write.mode(&quot;append&quot;).jdbc(&quot;jdbc:mysql://host:port/database&quot;,&quot;table&quot;,prop)</span><br></pre></td></tr></table></figure>\n","excerpt":"","more":"<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import java.util.Properties</span><br><span class=\"line\">val target_df = targetRdd.toDF()</span><br><span class=\"line\">val prop = new Properties()</span><br><span class=\"line\">prop.put(&quot;user&quot;, &quot;username&quot;)</span><br><span class=\"line\">prop.put(&quot;password&quot;, &quot;password&quot;)</span><br><span class=\"line\">ret_df.write.mode(&quot;append&quot;).jdbc(&quot;jdbc:mysql://host:port/database&quot;,&quot;table&quot;,prop)</span><br></pre></td></tr></table></figure>\n"},{"title":"机器学习相关材料","date":"2016-07-13T12:42:36.000Z","_content":"\n\n1、数学基础\n- 1、微积分 http://v.163.com/special/sp/singlevariablecalculus.html  http://open.163.com/special/opencourse/multivariable.html\n- 2、线性代数 http://open.163.com/special/opencourse/daishu.html\n- 3、概率统计http://open.163.com/special/Khan/probability.html\n想要深入学习机器学习，需要具有扎实的数学基础（矩阵分析和概率统计是基础的理论）\n\n2、进阶版本数学基础课程\n- 1、最优化理论（重点凸优化理论）  http://stanford.edu/~boyd/cvxbook/   http://xiaoyc.com/duality-theory-for-optimization/#1-1-lagrangian\n- 2、时变函数与泛函分析 http://ocw.nctu.edu.tw/course_list.php?page=2&bgid=1&gid=1  http://open.163.com/special/opencourse/fanhanfenxi.html\n- 3、随机过程 Hyperlink: Stochastic Process MIT View the complete course: Discrete Stochastic Processes\n\n3、机器学习基础课程\n- 1、Coursera上Andrew Ng Andrew Ng的《机器学习》\n- 2、林軒田(国立台湾大学) 機器學習基石\n\n4、机器学习基础课程\n- 1、 《统计学习方法》李航\n- 2、 《机器学习导论》\n- 3、 《PRML》\n- 4、 《图解机器学习》[日]杉山将\n- 5、   Machine Learning: A Probabilistic Prespective （Kevin Murphy）\n- 6、   Pattern Recognition and Machine Learning （Christopher Bishop）\n- 7、 《Spark机器学习》\n- 8、 《机器学习实战》\n- 9、 《贝叶斯思维:统计建模的Python学习法》\n- 10、Python自然语言处理》\n- 11、数学之美 （吴军）\n- 12、Web智能算法 （Haralambos Marmanis, Dmitry Babenko）\n- 13、集体智慧编程 （Toby Segaran）\n- 14、推荐系统实践\n- 15、计算广告学\n\n5、深度学习（注定将成为最近几年的爆发式增长）\n- 1、《Deep Learning》http://www.deeplearningbook.org\n- 2、  CSDN Blog http://blog.csdn.net/zouxy09/article/details/8775360\n- 3、《斯坦福大学深度学习教程》http://ufldl.stanford.edu/tutorial\n\n6、 杂项\n\n- 《Choosing a Machine Learning Classifier》\n- 《An Introduction to Deep Learning: From Perceptrons to Deep Networks》 译文：《http://www.cnblogs.com/xiaowanyer/p/3701944.html》\n- 《The LION Way: Machine Learning plus Intelligent Optimization》\n- 《分布式并行处理的数据》\n- 《Deep Learning for Natural Language Processing and Related Applications》\n- 《Neural Networks and Deep Learning》\n- 《分布式机器学习的故事》\n- 《Deep Learning 101》\n- 《Deep learning from the bottom up》\n- 《Deep Learning（深度学习）学习笔记整理系列》\n- 《Google Turns To Deep Learning Classification To Fight Web Spam》\n- 《Deep Learning Sentiment Analysis for Movie Reviews using Neo4j》\n- 《EMNLP上两篇关于股票趋势的应用论文 》\n- 《Learning to Rank for Information Retrieval and Natural Language Processing》\n- 《Geoffrey E. Hinton》\n- 《Andrej Karpathy的深度强化学习演示》 论文在这里\n- 《用大数据和机器学习做股票价格预测》\n- 《机器学习经典算法详解及Python实现--基于SMO的SVM分类器》\n- 《Use Google's Word2Vec for movie reviews》\n- 《深度卷积神经网络下围棋》\n- 《机器学习经典算法详解及Python实现--线性回归（Linear Regression）算法》\n- 《Caffe》\n- 《GoogLeNet深度学习模型的Caffe复现 》 GoogleNet论文\n- 《Deep Learning实战之word2vec》\n- 《A Deep Dive into Recurrent Neural Nets》\n- 《Geoffrey E. Hinton个人主页》\n- 《Deep Learning on Hadoop 2.0》\n- 《美团推荐算法实践》\n- 《The Trouble with SVMs》\n- 《Gaussian Processes for Machine Learning》\n- 《Introduction to ARMA Time Series Models – simplified》\n- 《Neural Net in C++ Tutorial》\n- 《Deep Learning Tutorials》\n- 《Deep Learning, The Curse of Dimensionality, and Autoencoders》\n- 《Topic modeling with LDA: MLlib meets GraphX》\n- 《Deep Learning for Multi-label Classification》\n- 《Google DeepMind publications》  AlphaGo团队官方论文\n- 《AM207: Monte Carlo Methods, Stochastic Optimization》\n- 《Back-to-Basics Weekend Reading - Machine Learning》\n- 《A Probabilistic Theory of Deep Learning》\n- 《How does Quora use machine learning in 2015?》\n- 《Parallel Machine Learning with scikit-learn and IPython》\n- 《Time Series Econometrics - A Concise Course》\n- 《A comparison of open source tools for sentiment analysis》\n- 《International Joint Conference on Artificial Intelligence Accepted paper》\n- 《How to Evaluate Machine Learning Models, Part 1: Orientation》 How to Evaluate Machine Learning Models, Part 2a: Classification Metrics,How to Evaluate Machine Learning Models, Part 2b: Ranking and Regression Metrics.\n- 《Learning scikit-learn: Machine Learning in Python》\n- 《Lightning fast Machine Learning with Spark》\n- 《How we’re using machine learning to fight shell selling》\n- 《Mining of Massive Datasets》\n- 《Advances in Extreme Learning Machines》\n- 《The Curse of Dimensionality in classification》\n- 《Demistifying LSTM Neural Networks》\n- 《Decoding Dimensionality Reduction, PCA and SVD》\n- 《What are the advantages of different classification algorithms?》\n- 《Kaggle R Tutorial on Machine Learning》 《Interactive R Tutorial: Machine Learning for the Titanic Competition》.\n- 《Logistic Regression and Gradient Descent》\n- 《Stock Forecasting With Machine Learning - Seven Possible Errors》\n- 《LR原理解析》  http://www.cnblogs.com/xiaowanyer/p/3701944.html\n- 机器学习顶级会议和杂志 http://icml.cc/2015/?page_id=175\n- 贝耶斯回归材料 http://blog.csdn.net/haoni123321/article/details/37913795\n- 贝耶斯回归材料 http://mindhacks.cn/2008/09/21/the-magical-bayesian-method/ \n- AlphaGo原理解析https://www.youtube.com/watch?v=63FDxJ5e_Ew\n\n","source":"_posts/机器学习相关材料.md","raw":"---\ntitle: 机器学习相关材料\ndate: 2016-07-13 20:42:36\ntags:\n---\n\n\n1、数学基础\n- 1、微积分 http://v.163.com/special/sp/singlevariablecalculus.html  http://open.163.com/special/opencourse/multivariable.html\n- 2、线性代数 http://open.163.com/special/opencourse/daishu.html\n- 3、概率统计http://open.163.com/special/Khan/probability.html\n想要深入学习机器学习，需要具有扎实的数学基础（矩阵分析和概率统计是基础的理论）\n\n2、进阶版本数学基础课程\n- 1、最优化理论（重点凸优化理论）  http://stanford.edu/~boyd/cvxbook/   http://xiaoyc.com/duality-theory-for-optimization/#1-1-lagrangian\n- 2、时变函数与泛函分析 http://ocw.nctu.edu.tw/course_list.php?page=2&bgid=1&gid=1  http://open.163.com/special/opencourse/fanhanfenxi.html\n- 3、随机过程 Hyperlink: Stochastic Process MIT View the complete course: Discrete Stochastic Processes\n\n3、机器学习基础课程\n- 1、Coursera上Andrew Ng Andrew Ng的《机器学习》\n- 2、林軒田(国立台湾大学) 機器學習基石\n\n4、机器学习基础课程\n- 1、 《统计学习方法》李航\n- 2、 《机器学习导论》\n- 3、 《PRML》\n- 4、 《图解机器学习》[日]杉山将\n- 5、   Machine Learning: A Probabilistic Prespective （Kevin Murphy）\n- 6、   Pattern Recognition and Machine Learning （Christopher Bishop）\n- 7、 《Spark机器学习》\n- 8、 《机器学习实战》\n- 9、 《贝叶斯思维:统计建模的Python学习法》\n- 10、Python自然语言处理》\n- 11、数学之美 （吴军）\n- 12、Web智能算法 （Haralambos Marmanis, Dmitry Babenko）\n- 13、集体智慧编程 （Toby Segaran）\n- 14、推荐系统实践\n- 15、计算广告学\n\n5、深度学习（注定将成为最近几年的爆发式增长）\n- 1、《Deep Learning》http://www.deeplearningbook.org\n- 2、  CSDN Blog http://blog.csdn.net/zouxy09/article/details/8775360\n- 3、《斯坦福大学深度学习教程》http://ufldl.stanford.edu/tutorial\n\n6、 杂项\n\n- 《Choosing a Machine Learning Classifier》\n- 《An Introduction to Deep Learning: From Perceptrons to Deep Networks》 译文：《http://www.cnblogs.com/xiaowanyer/p/3701944.html》\n- 《The LION Way: Machine Learning plus Intelligent Optimization》\n- 《分布式并行处理的数据》\n- 《Deep Learning for Natural Language Processing and Related Applications》\n- 《Neural Networks and Deep Learning》\n- 《分布式机器学习的故事》\n- 《Deep Learning 101》\n- 《Deep learning from the bottom up》\n- 《Deep Learning（深度学习）学习笔记整理系列》\n- 《Google Turns To Deep Learning Classification To Fight Web Spam》\n- 《Deep Learning Sentiment Analysis for Movie Reviews using Neo4j》\n- 《EMNLP上两篇关于股票趋势的应用论文 》\n- 《Learning to Rank for Information Retrieval and Natural Language Processing》\n- 《Geoffrey E. Hinton》\n- 《Andrej Karpathy的深度强化学习演示》 论文在这里\n- 《用大数据和机器学习做股票价格预测》\n- 《机器学习经典算法详解及Python实现--基于SMO的SVM分类器》\n- 《Use Google's Word2Vec for movie reviews》\n- 《深度卷积神经网络下围棋》\n- 《机器学习经典算法详解及Python实现--线性回归（Linear Regression）算法》\n- 《Caffe》\n- 《GoogLeNet深度学习模型的Caffe复现 》 GoogleNet论文\n- 《Deep Learning实战之word2vec》\n- 《A Deep Dive into Recurrent Neural Nets》\n- 《Geoffrey E. Hinton个人主页》\n- 《Deep Learning on Hadoop 2.0》\n- 《美团推荐算法实践》\n- 《The Trouble with SVMs》\n- 《Gaussian Processes for Machine Learning》\n- 《Introduction to ARMA Time Series Models – simplified》\n- 《Neural Net in C++ Tutorial》\n- 《Deep Learning Tutorials》\n- 《Deep Learning, The Curse of Dimensionality, and Autoencoders》\n- 《Topic modeling with LDA: MLlib meets GraphX》\n- 《Deep Learning for Multi-label Classification》\n- 《Google DeepMind publications》  AlphaGo团队官方论文\n- 《AM207: Monte Carlo Methods, Stochastic Optimization》\n- 《Back-to-Basics Weekend Reading - Machine Learning》\n- 《A Probabilistic Theory of Deep Learning》\n- 《How does Quora use machine learning in 2015?》\n- 《Parallel Machine Learning with scikit-learn and IPython》\n- 《Time Series Econometrics - A Concise Course》\n- 《A comparison of open source tools for sentiment analysis》\n- 《International Joint Conference on Artificial Intelligence Accepted paper》\n- 《How to Evaluate Machine Learning Models, Part 1: Orientation》 How to Evaluate Machine Learning Models, Part 2a: Classification Metrics,How to Evaluate Machine Learning Models, Part 2b: Ranking and Regression Metrics.\n- 《Learning scikit-learn: Machine Learning in Python》\n- 《Lightning fast Machine Learning with Spark》\n- 《How we’re using machine learning to fight shell selling》\n- 《Mining of Massive Datasets》\n- 《Advances in Extreme Learning Machines》\n- 《The Curse of Dimensionality in classification》\n- 《Demistifying LSTM Neural Networks》\n- 《Decoding Dimensionality Reduction, PCA and SVD》\n- 《What are the advantages of different classification algorithms?》\n- 《Kaggle R Tutorial on Machine Learning》 《Interactive R Tutorial: Machine Learning for the Titanic Competition》.\n- 《Logistic Regression and Gradient Descent》\n- 《Stock Forecasting With Machine Learning - Seven Possible Errors》\n- 《LR原理解析》  http://www.cnblogs.com/xiaowanyer/p/3701944.html\n- 机器学习顶级会议和杂志 http://icml.cc/2015/?page_id=175\n- 贝耶斯回归材料 http://blog.csdn.net/haoni123321/article/details/37913795\n- 贝耶斯回归材料 http://mindhacks.cn/2008/09/21/the-magical-bayesian-method/ \n- AlphaGo原理解析https://www.youtube.com/watch?v=63FDxJ5e_Ew\n\n","slug":"机器学习相关材料","published":1,"updated":"2016-12-10T14:11:24.624Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciwjb1ssg000udag3eezsf02f","content":"<p>1、数学基础</p>\n<ul>\n<li>1、微积分 <a href=\"http://v.163.com/special/sp/singlevariablecalculus.html\" target=\"_blank\" rel=\"external\">http://v.163.com/special/sp/singlevariablecalculus.html</a>  <a href=\"http://open.163.com/special/opencourse/multivariable.html\" target=\"_blank\" rel=\"external\">http://open.163.com/special/opencourse/multivariable.html</a></li>\n<li>2、线性代数 <a href=\"http://open.163.com/special/opencourse/daishu.html\" target=\"_blank\" rel=\"external\">http://open.163.com/special/opencourse/daishu.html</a></li>\n<li>3、概率统计<a href=\"http://open.163.com/special/Khan/probability.html\" target=\"_blank\" rel=\"external\">http://open.163.com/special/Khan/probability.html</a><br>想要深入学习机器学习，需要具有扎实的数学基础（矩阵分析和概率统计是基础的理论）</li>\n</ul>\n<p>2、进阶版本数学基础课程</p>\n<ul>\n<li>1、最优化理论（重点凸优化理论）  <a href=\"http://stanford.edu/~boyd/cvxbook/\" target=\"_blank\" rel=\"external\">http://stanford.edu/~boyd/cvxbook/</a>   <a href=\"http://xiaoyc.com/duality-theory-for-optimization/#1-1-lagrangian\" target=\"_blank\" rel=\"external\">http://xiaoyc.com/duality-theory-for-optimization/#1-1-lagrangian</a></li>\n<li>2、时变函数与泛函分析 <a href=\"http://ocw.nctu.edu.tw/course_list.php?page=2&amp;bgid=1&amp;gid=1\" target=\"_blank\" rel=\"external\">http://ocw.nctu.edu.tw/course_list.php?page=2&amp;bgid=1&amp;gid=1</a>  <a href=\"http://open.163.com/special/opencourse/fanhanfenxi.html\" target=\"_blank\" rel=\"external\">http://open.163.com/special/opencourse/fanhanfenxi.html</a></li>\n<li>3、随机过程 Hyperlink: Stochastic Process MIT View the complete course: Discrete Stochastic Processes</li>\n</ul>\n<p>3、机器学习基础课程</p>\n<ul>\n<li>1、Coursera上Andrew Ng Andrew Ng的《机器学习》</li>\n<li>2、林軒田(国立台湾大学) 機器學習基石</li>\n</ul>\n<p>4、机器学习基础课程</p>\n<ul>\n<li>1、 《统计学习方法》李航</li>\n<li>2、 《机器学习导论》</li>\n<li>3、 《PRML》</li>\n<li>4、 《图解机器学习》[日]杉山将</li>\n<li>5、   Machine Learning: A Probabilistic Prespective （Kevin Murphy）</li>\n<li>6、   Pattern Recognition and Machine Learning （Christopher Bishop）</li>\n<li>7、 《Spark机器学习》</li>\n<li>8、 《机器学习实战》</li>\n<li>9、 《贝叶斯思维:统计建模的Python学习法》</li>\n<li>10、Python自然语言处理》</li>\n<li>11、数学之美 （吴军）</li>\n<li>12、Web智能算法 （Haralambos Marmanis, Dmitry Babenko）</li>\n<li>13、集体智慧编程 （Toby Segaran）</li>\n<li>14、推荐系统实践</li>\n<li>15、计算广告学</li>\n</ul>\n<p>5、深度学习（注定将成为最近几年的爆发式增长）</p>\n<ul>\n<li>1、《Deep Learning》<a href=\"http://www.deeplearningbook.org\" target=\"_blank\" rel=\"external\">http://www.deeplearningbook.org</a></li>\n<li>2、  CSDN Blog <a href=\"http://blog.csdn.net/zouxy09/article/details/8775360\" target=\"_blank\" rel=\"external\">http://blog.csdn.net/zouxy09/article/details/8775360</a></li>\n<li>3、《斯坦福大学深度学习教程》<a href=\"http://ufldl.stanford.edu/tutorial\" target=\"_blank\" rel=\"external\">http://ufldl.stanford.edu/tutorial</a></li>\n</ul>\n<p>6、 杂项</p>\n<ul>\n<li>《Choosing a Machine Learning Classifier》</li>\n<li>《An Introduction to Deep Learning: From Perceptrons to Deep Networks》 译文：《<a href=\"http://www.cnblogs.com/xiaowanyer/p/3701944.html》\" target=\"_blank\" rel=\"external\">http://www.cnblogs.com/xiaowanyer/p/3701944.html》</a></li>\n<li>《The LION Way: Machine Learning plus Intelligent Optimization》</li>\n<li>《分布式并行处理的数据》</li>\n<li>《Deep Learning for Natural Language Processing and Related Applications》</li>\n<li>《Neural Networks and Deep Learning》</li>\n<li>《分布式机器学习的故事》</li>\n<li>《Deep Learning 101》</li>\n<li>《Deep learning from the bottom up》</li>\n<li>《Deep Learning（深度学习）学习笔记整理系列》</li>\n<li>《Google Turns To Deep Learning Classification To Fight Web Spam》</li>\n<li>《Deep Learning Sentiment Analysis for Movie Reviews using Neo4j》</li>\n<li>《EMNLP上两篇关于股票趋势的应用论文 》</li>\n<li>《Learning to Rank for Information Retrieval and Natural Language Processing》</li>\n<li>《Geoffrey E. Hinton》</li>\n<li>《Andrej Karpathy的深度强化学习演示》 论文在这里</li>\n<li>《用大数据和机器学习做股票价格预测》</li>\n<li>《机器学习经典算法详解及Python实现–基于SMO的SVM分类器》</li>\n<li>《Use Google’s Word2Vec for movie reviews》</li>\n<li>《深度卷积神经网络下围棋》</li>\n<li>《机器学习经典算法详解及Python实现–线性回归（Linear Regression）算法》</li>\n<li>《Caffe》</li>\n<li>《GoogLeNet深度学习模型的Caffe复现 》 GoogleNet论文</li>\n<li>《Deep Learning实战之word2vec》</li>\n<li>《A Deep Dive into Recurrent Neural Nets》</li>\n<li>《Geoffrey E. Hinton个人主页》</li>\n<li>《Deep Learning on Hadoop 2.0》</li>\n<li>《美团推荐算法实践》</li>\n<li>《The Trouble with SVMs》</li>\n<li>《Gaussian Processes for Machine Learning》</li>\n<li>《Introduction to ARMA Time Series Models – simplified》</li>\n<li>《Neural Net in C++ Tutorial》</li>\n<li>《Deep Learning Tutorials》</li>\n<li>《Deep Learning, The Curse of Dimensionality, and Autoencoders》</li>\n<li>《Topic modeling with LDA: MLlib meets GraphX》</li>\n<li>《Deep Learning for Multi-label Classification》</li>\n<li>《Google DeepMind publications》  AlphaGo团队官方论文</li>\n<li>《AM207: Monte Carlo Methods, Stochastic Optimization》</li>\n<li>《Back-to-Basics Weekend Reading - Machine Learning》</li>\n<li>《A Probabilistic Theory of Deep Learning》</li>\n<li>《How does Quora use machine learning in 2015?》</li>\n<li>《Parallel Machine Learning with scikit-learn and IPython》</li>\n<li>《Time Series Econometrics - A Concise Course》</li>\n<li>《A comparison of open source tools for sentiment analysis》</li>\n<li>《International Joint Conference on Artificial Intelligence Accepted paper》</li>\n<li>《How to Evaluate Machine Learning Models, Part 1: Orientation》 How to Evaluate Machine Learning Models, Part 2a: Classification Metrics,How to Evaluate Machine Learning Models, Part 2b: Ranking and Regression Metrics.</li>\n<li>《Learning scikit-learn: Machine Learning in Python》</li>\n<li>《Lightning fast Machine Learning with Spark》</li>\n<li>《How we’re using machine learning to fight shell selling》</li>\n<li>《Mining of Massive Datasets》</li>\n<li>《Advances in Extreme Learning Machines》</li>\n<li>《The Curse of Dimensionality in classification》</li>\n<li>《Demistifying LSTM Neural Networks》</li>\n<li>《Decoding Dimensionality Reduction, PCA and SVD》</li>\n<li>《What are the advantages of different classification algorithms?》</li>\n<li>《Kaggle R Tutorial on Machine Learning》 《Interactive R Tutorial: Machine Learning for the Titanic Competition》.</li>\n<li>《Logistic Regression and Gradient Descent》</li>\n<li>《Stock Forecasting With Machine Learning - Seven Possible Errors》</li>\n<li>《LR原理解析》  <a href=\"http://www.cnblogs.com/xiaowanyer/p/3701944.html\" target=\"_blank\" rel=\"external\">http://www.cnblogs.com/xiaowanyer/p/3701944.html</a></li>\n<li>机器学习顶级会议和杂志 <a href=\"http://icml.cc/2015/?page_id=175\" target=\"_blank\" rel=\"external\">http://icml.cc/2015/?page_id=175</a></li>\n<li>贝耶斯回归材料 <a href=\"http://blog.csdn.net/haoni123321/article/details/37913795\" target=\"_blank\" rel=\"external\">http://blog.csdn.net/haoni123321/article/details/37913795</a></li>\n<li>贝耶斯回归材料 <a href=\"http://mindhacks.cn/2008/09/21/the-magical-bayesian-method/\" target=\"_blank\" rel=\"external\">http://mindhacks.cn/2008/09/21/the-magical-bayesian-method/</a> </li>\n<li>AlphaGo原理解析<a href=\"https://www.youtube.com/watch?v=63FDxJ5e_Ew\" target=\"_blank\" rel=\"external\">https://www.youtube.com/watch?v=63FDxJ5e_Ew</a></li>\n</ul>\n","excerpt":"","more":"<p>1、数学基础</p>\n<ul>\n<li>1、微积分 <a href=\"http://v.163.com/special/sp/singlevariablecalculus.html\">http://v.163.com/special/sp/singlevariablecalculus.html</a>  <a href=\"http://open.163.com/special/opencourse/multivariable.html\">http://open.163.com/special/opencourse/multivariable.html</a></li>\n<li>2、线性代数 <a href=\"http://open.163.com/special/opencourse/daishu.html\">http://open.163.com/special/opencourse/daishu.html</a></li>\n<li>3、概率统计<a href=\"http://open.163.com/special/Khan/probability.html\">http://open.163.com/special/Khan/probability.html</a><br>想要深入学习机器学习，需要具有扎实的数学基础（矩阵分析和概率统计是基础的理论）</li>\n</ul>\n<p>2、进阶版本数学基础课程</p>\n<ul>\n<li>1、最优化理论（重点凸优化理论）  <a href=\"http://stanford.edu/~boyd/cvxbook/\">http://stanford.edu/~boyd/cvxbook/</a>   <a href=\"http://xiaoyc.com/duality-theory-for-optimization/#1-1-lagrangian\">http://xiaoyc.com/duality-theory-for-optimization/#1-1-lagrangian</a></li>\n<li>2、时变函数与泛函分析 <a href=\"http://ocw.nctu.edu.tw/course_list.php?page=2&amp;bgid=1&amp;gid=1\">http://ocw.nctu.edu.tw/course_list.php?page=2&amp;bgid=1&amp;gid=1</a>  <a href=\"http://open.163.com/special/opencourse/fanhanfenxi.html\">http://open.163.com/special/opencourse/fanhanfenxi.html</a></li>\n<li>3、随机过程 Hyperlink: Stochastic Process MIT View the complete course: Discrete Stochastic Processes</li>\n</ul>\n<p>3、机器学习基础课程</p>\n<ul>\n<li>1、Coursera上Andrew Ng Andrew Ng的《机器学习》</li>\n<li>2、林軒田(国立台湾大学) 機器學習基石</li>\n</ul>\n<p>4、机器学习基础课程</p>\n<ul>\n<li>1、 《统计学习方法》李航</li>\n<li>2、 《机器学习导论》</li>\n<li>3、 《PRML》</li>\n<li>4、 《图解机器学习》[日]杉山将</li>\n<li>5、   Machine Learning: A Probabilistic Prespective （Kevin Murphy）</li>\n<li>6、   Pattern Recognition and Machine Learning （Christopher Bishop）</li>\n<li>7、 《Spark机器学习》</li>\n<li>8、 《机器学习实战》</li>\n<li>9、 《贝叶斯思维:统计建模的Python学习法》</li>\n<li>10、Python自然语言处理》</li>\n<li>11、数学之美 （吴军）</li>\n<li>12、Web智能算法 （Haralambos Marmanis, Dmitry Babenko）</li>\n<li>13、集体智慧编程 （Toby Segaran）</li>\n<li>14、推荐系统实践</li>\n<li>15、计算广告学</li>\n</ul>\n<p>5、深度学习（注定将成为最近几年的爆发式增长）</p>\n<ul>\n<li>1、《Deep Learning》<a href=\"http://www.deeplearningbook.org\">http://www.deeplearningbook.org</a></li>\n<li>2、  CSDN Blog <a href=\"http://blog.csdn.net/zouxy09/article/details/8775360\">http://blog.csdn.net/zouxy09/article/details/8775360</a></li>\n<li>3、《斯坦福大学深度学习教程》<a href=\"http://ufldl.stanford.edu/tutorial\">http://ufldl.stanford.edu/tutorial</a></li>\n</ul>\n<p>6、 杂项</p>\n<ul>\n<li>《Choosing a Machine Learning Classifier》</li>\n<li>《An Introduction to Deep Learning: From Perceptrons to Deep Networks》 译文：《<a href=\"http://www.cnblogs.com/xiaowanyer/p/3701944.html》\">http://www.cnblogs.com/xiaowanyer/p/3701944.html》</a></li>\n<li>《The LION Way: Machine Learning plus Intelligent Optimization》</li>\n<li>《分布式并行处理的数据》</li>\n<li>《Deep Learning for Natural Language Processing and Related Applications》</li>\n<li>《Neural Networks and Deep Learning》</li>\n<li>《分布式机器学习的故事》</li>\n<li>《Deep Learning 101》</li>\n<li>《Deep learning from the bottom up》</li>\n<li>《Deep Learning（深度学习）学习笔记整理系列》</li>\n<li>《Google Turns To Deep Learning Classification To Fight Web Spam》</li>\n<li>《Deep Learning Sentiment Analysis for Movie Reviews using Neo4j》</li>\n<li>《EMNLP上两篇关于股票趋势的应用论文 》</li>\n<li>《Learning to Rank for Information Retrieval and Natural Language Processing》</li>\n<li>《Geoffrey E. Hinton》</li>\n<li>《Andrej Karpathy的深度强化学习演示》 论文在这里</li>\n<li>《用大数据和机器学习做股票价格预测》</li>\n<li>《机器学习经典算法详解及Python实现–基于SMO的SVM分类器》</li>\n<li>《Use Google’s Word2Vec for movie reviews》</li>\n<li>《深度卷积神经网络下围棋》</li>\n<li>《机器学习经典算法详解及Python实现–线性回归（Linear Regression）算法》</li>\n<li>《Caffe》</li>\n<li>《GoogLeNet深度学习模型的Caffe复现 》 GoogleNet论文</li>\n<li>《Deep Learning实战之word2vec》</li>\n<li>《A Deep Dive into Recurrent Neural Nets》</li>\n<li>《Geoffrey E. Hinton个人主页》</li>\n<li>《Deep Learning on Hadoop 2.0》</li>\n<li>《美团推荐算法实践》</li>\n<li>《The Trouble with SVMs》</li>\n<li>《Gaussian Processes for Machine Learning》</li>\n<li>《Introduction to ARMA Time Series Models – simplified》</li>\n<li>《Neural Net in C++ Tutorial》</li>\n<li>《Deep Learning Tutorials》</li>\n<li>《Deep Learning, The Curse of Dimensionality, and Autoencoders》</li>\n<li>《Topic modeling with LDA: MLlib meets GraphX》</li>\n<li>《Deep Learning for Multi-label Classification》</li>\n<li>《Google DeepMind publications》  AlphaGo团队官方论文</li>\n<li>《AM207: Monte Carlo Methods, Stochastic Optimization》</li>\n<li>《Back-to-Basics Weekend Reading - Machine Learning》</li>\n<li>《A Probabilistic Theory of Deep Learning》</li>\n<li>《How does Quora use machine learning in 2015?》</li>\n<li>《Parallel Machine Learning with scikit-learn and IPython》</li>\n<li>《Time Series Econometrics - A Concise Course》</li>\n<li>《A comparison of open source tools for sentiment analysis》</li>\n<li>《International Joint Conference on Artificial Intelligence Accepted paper》</li>\n<li>《How to Evaluate Machine Learning Models, Part 1: Orientation》 How to Evaluate Machine Learning Models, Part 2a: Classification Metrics,How to Evaluate Machine Learning Models, Part 2b: Ranking and Regression Metrics.</li>\n<li>《Learning scikit-learn: Machine Learning in Python》</li>\n<li>《Lightning fast Machine Learning with Spark》</li>\n<li>《How we’re using machine learning to fight shell selling》</li>\n<li>《Mining of Massive Datasets》</li>\n<li>《Advances in Extreme Learning Machines》</li>\n<li>《The Curse of Dimensionality in classification》</li>\n<li>《Demistifying LSTM Neural Networks》</li>\n<li>《Decoding Dimensionality Reduction, PCA and SVD》</li>\n<li>《What are the advantages of different classification algorithms?》</li>\n<li>《Kaggle R Tutorial on Machine Learning》 《Interactive R Tutorial: Machine Learning for the Titanic Competition》.</li>\n<li>《Logistic Regression and Gradient Descent》</li>\n<li>《Stock Forecasting With Machine Learning - Seven Possible Errors》</li>\n<li>《LR原理解析》  <a href=\"http://www.cnblogs.com/xiaowanyer/p/3701944.html\">http://www.cnblogs.com/xiaowanyer/p/3701944.html</a></li>\n<li>机器学习顶级会议和杂志 <a href=\"http://icml.cc/2015/?page_id=175\">http://icml.cc/2015/?page_id=175</a></li>\n<li>贝耶斯回归材料 <a href=\"http://blog.csdn.net/haoni123321/article/details/37913795\">http://blog.csdn.net/haoni123321/article/details/37913795</a></li>\n<li>贝耶斯回归材料 <a href=\"http://mindhacks.cn/2008/09/21/the-magical-bayesian-method/\">http://mindhacks.cn/2008/09/21/the-magical-bayesian-method/</a> </li>\n<li>AlphaGo原理解析<a href=\"https://www.youtube.com/watch?v=63FDxJ5e_Ew\">https://www.youtube.com/watch?v=63FDxJ5e_Ew</a></li>\n</ul>\n"},{"title":"过拟合的原因","date":"2016-07-13T05:46:10.000Z","_content":"# 过拟合产生原因\n\n## 观测数据存在误差\n用于训练的样本数据，即观测数据因为各种个样的原因，总会产生误差。如果选择的假设过分追求能够完美解释观测数据（对于回归问题，可能是拟合曲线“穿过”所有的样本点，或者类似于均方误差过小），都有可能造成过拟合的现象。造成这种现象的根本原因在于，拟合曲线把误差也完美的学习了。\n\n## 产生样本因素很多，但是实际可能只有小部分提取出来\n影响产生观测数据的因素有很多，但是现实中可能仅提取几个和结果相关度很高的因素来进行分析。这个时候观察数据会倾向于围绕你的有限模型的预测结果呈正态分布，于是你实际观察到的结果就是这个正态分布的随机取样，这个取样很可能受到其余因素的影响偏离你的模型所预测的中心，这个时候便不能贪心不足地试图通过改变模型来“完美”匹配数据，因为那些使结果偏离你的预测的贡献因素不是你这个有限模型里面含有的因素所能概括的，硬要打肿脸充胖子只能导致不实际的模型。\n\n## 奥卡姆剃刀法则\n$$\np\\left( h|D\\right )   = p\\left( h\\right ) p\\left( D|h\\right ) \n$$\n高卡姆剃刀法则的含义是如果存在多个假设和观察一致，则应当选择最简单的那一个。最简单的假设意味着$\\left( h\\right)$较大，而与观测一致，意味着似然数值较大，即$p\\left( D|h\\right )$较大。\n\n奥卡姆剃刀法则青睐于先验概率，认为先验较大的模型有较大的优势；最大似然法则认为似然大的模型具有较大的优势；而贝叶斯法则则认为二者乘积决定模型的选择问题。\n\n## 贝叶斯奥卡姆剃刀\n上面的奥卡姆剃刀法则描述的是传统的剃刀法则，主要指先验概率$\\left( h\\right)$,而贝叶斯法奥卡姆剃刀法其实和似然$p\\left( D|h\\right )$上面，即该法则主要衡量的因素是似然本身出现的概率大小。 \n","source":"_posts/过拟合的原因.md","raw":"---\ntitle: 过拟合的原因\ndate: 2016-07-13 13:46:10\ntags:\n---\n# 过拟合产生原因\n\n## 观测数据存在误差\n用于训练的样本数据，即观测数据因为各种个样的原因，总会产生误差。如果选择的假设过分追求能够完美解释观测数据（对于回归问题，可能是拟合曲线“穿过”所有的样本点，或者类似于均方误差过小），都有可能造成过拟合的现象。造成这种现象的根本原因在于，拟合曲线把误差也完美的学习了。\n\n## 产生样本因素很多，但是实际可能只有小部分提取出来\n影响产生观测数据的因素有很多，但是现实中可能仅提取几个和结果相关度很高的因素来进行分析。这个时候观察数据会倾向于围绕你的有限模型的预测结果呈正态分布，于是你实际观察到的结果就是这个正态分布的随机取样，这个取样很可能受到其余因素的影响偏离你的模型所预测的中心，这个时候便不能贪心不足地试图通过改变模型来“完美”匹配数据，因为那些使结果偏离你的预测的贡献因素不是你这个有限模型里面含有的因素所能概括的，硬要打肿脸充胖子只能导致不实际的模型。\n\n## 奥卡姆剃刀法则\n$$\np\\left( h|D\\right )   = p\\left( h\\right ) p\\left( D|h\\right ) \n$$\n高卡姆剃刀法则的含义是如果存在多个假设和观察一致，则应当选择最简单的那一个。最简单的假设意味着$\\left( h\\right)$较大，而与观测一致，意味着似然数值较大，即$p\\left( D|h\\right )$较大。\n\n奥卡姆剃刀法则青睐于先验概率，认为先验较大的模型有较大的优势；最大似然法则认为似然大的模型具有较大的优势；而贝叶斯法则则认为二者乘积决定模型的选择问题。\n\n## 贝叶斯奥卡姆剃刀\n上面的奥卡姆剃刀法则描述的是传统的剃刀法则，主要指先验概率$\\left( h\\right)$,而贝叶斯法奥卡姆剃刀法其实和似然$p\\left( D|h\\right )$上面，即该法则主要衡量的因素是似然本身出现的概率大小。 \n","slug":"过拟合的原因","published":1,"updated":"2016-12-10T14:11:24.624Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciwjb1ssl000wdag381t52t4q","content":"<h1 id=\"过拟合产生原因\"><a href=\"#过拟合产生原因\" class=\"headerlink\" title=\"过拟合产生原因\"></a>过拟合产生原因</h1><h2 id=\"观测数据存在误差\"><a href=\"#观测数据存在误差\" class=\"headerlink\" title=\"观测数据存在误差\"></a>观测数据存在误差</h2><p>用于训练的样本数据，即观测数据因为各种个样的原因，总会产生误差。如果选择的假设过分追求能够完美解释观测数据（对于回归问题，可能是拟合曲线“穿过”所有的样本点，或者类似于均方误差过小），都有可能造成过拟合的现象。造成这种现象的根本原因在于，拟合曲线把误差也完美的学习了。</p>\n<h2 id=\"产生样本因素很多，但是实际可能只有小部分提取出来\"><a href=\"#产生样本因素很多，但是实际可能只有小部分提取出来\" class=\"headerlink\" title=\"产生样本因素很多，但是实际可能只有小部分提取出来\"></a>产生样本因素很多，但是实际可能只有小部分提取出来</h2><p>影响产生观测数据的因素有很多，但是现实中可能仅提取几个和结果相关度很高的因素来进行分析。这个时候观察数据会倾向于围绕你的有限模型的预测结果呈正态分布，于是你实际观察到的结果就是这个正态分布的随机取样，这个取样很可能受到其余因素的影响偏离你的模型所预测的中心，这个时候便不能贪心不足地试图通过改变模型来“完美”匹配数据，因为那些使结果偏离你的预测的贡献因素不是你这个有限模型里面含有的因素所能概括的，硬要打肿脸充胖子只能导致不实际的模型。</p>\n<h2 id=\"奥卡姆剃刀法则\"><a href=\"#奥卡姆剃刀法则\" class=\"headerlink\" title=\"奥卡姆剃刀法则\"></a>奥卡姆剃刀法则</h2><p>$$<br>p\\left( h|D\\right )   = p\\left( h\\right ) p\\left( D|h\\right )<br>$$<br>高卡姆剃刀法则的含义是如果存在多个假设和观察一致，则应当选择最简单的那一个。最简单的假设意味着$\\left( h\\right)$较大，而与观测一致，意味着似然数值较大，即$p\\left( D|h\\right )$较大。</p>\n<p>奥卡姆剃刀法则青睐于先验概率，认为先验较大的模型有较大的优势；最大似然法则认为似然大的模型具有较大的优势；而贝叶斯法则则认为二者乘积决定模型的选择问题。</p>\n<h2 id=\"贝叶斯奥卡姆剃刀\"><a href=\"#贝叶斯奥卡姆剃刀\" class=\"headerlink\" title=\"贝叶斯奥卡姆剃刀\"></a>贝叶斯奥卡姆剃刀</h2><p>上面的奥卡姆剃刀法则描述的是传统的剃刀法则，主要指先验概率$\\left( h\\right)$,而贝叶斯法奥卡姆剃刀法其实和似然$p\\left( D|h\\right )$上面，即该法则主要衡量的因素是似然本身出现的概率大小。 </p>\n","excerpt":"","more":"<h1 id=\"过拟合产生原因\"><a href=\"#过拟合产生原因\" class=\"headerlink\" title=\"过拟合产生原因\"></a>过拟合产生原因</h1><h2 id=\"观测数据存在误差\"><a href=\"#观测数据存在误差\" class=\"headerlink\" title=\"观测数据存在误差\"></a>观测数据存在误差</h2><p>用于训练的样本数据，即观测数据因为各种个样的原因，总会产生误差。如果选择的假设过分追求能够完美解释观测数据（对于回归问题，可能是拟合曲线“穿过”所有的样本点，或者类似于均方误差过小），都有可能造成过拟合的现象。造成这种现象的根本原因在于，拟合曲线把误差也完美的学习了。</p>\n<h2 id=\"产生样本因素很多，但是实际可能只有小部分提取出来\"><a href=\"#产生样本因素很多，但是实际可能只有小部分提取出来\" class=\"headerlink\" title=\"产生样本因素很多，但是实际可能只有小部分提取出来\"></a>产生样本因素很多，但是实际可能只有小部分提取出来</h2><p>影响产生观测数据的因素有很多，但是现实中可能仅提取几个和结果相关度很高的因素来进行分析。这个时候观察数据会倾向于围绕你的有限模型的预测结果呈正态分布，于是你实际观察到的结果就是这个正态分布的随机取样，这个取样很可能受到其余因素的影响偏离你的模型所预测的中心，这个时候便不能贪心不足地试图通过改变模型来“完美”匹配数据，因为那些使结果偏离你的预测的贡献因素不是你这个有限模型里面含有的因素所能概括的，硬要打肿脸充胖子只能导致不实际的模型。</p>\n<h2 id=\"奥卡姆剃刀法则\"><a href=\"#奥卡姆剃刀法则\" class=\"headerlink\" title=\"奥卡姆剃刀法则\"></a>奥卡姆剃刀法则</h2><p>$$<br>p\\left( h|D\\right )   = p\\left( h\\right ) p\\left( D|h\\right )<br>$$<br>高卡姆剃刀法则的含义是如果存在多个假设和观察一致，则应当选择最简单的那一个。最简单的假设意味着$\\left( h\\right)$较大，而与观测一致，意味着似然数值较大，即$p\\left( D|h\\right )$较大。</p>\n<p>奥卡姆剃刀法则青睐于先验概率，认为先验较大的模型有较大的优势；最大似然法则认为似然大的模型具有较大的优势；而贝叶斯法则则认为二者乘积决定模型的选择问题。</p>\n<h2 id=\"贝叶斯奥卡姆剃刀\"><a href=\"#贝叶斯奥卡姆剃刀\" class=\"headerlink\" title=\"贝叶斯奥卡姆剃刀\"></a>贝叶斯奥卡姆剃刀</h2><p>上面的奥卡姆剃刀法则描述的是传统的剃刀法则，主要指先验概率$\\left( h\\right)$,而贝叶斯法奥卡姆剃刀法其实和似然$p\\left( D|h\\right )$上面，即该法则主要衡量的因素是似然本身出现的概率大小。 </p>\n"}],"PostAsset":[],"PostCategory":[],"PostTag":[{"post_id":"ciwjb1sq10000dag3zsdlzbx3","tag_id":"ciwjb1sqi0003dag30le4fp9p","_id":"ciwjb1sqz0008dag3hc5gf06v"},{"post_id":"ciwjb1sqs0005dag390421a88","tag_id":"ciwjb1sqy0007dag3wdnwr1kl","_id":"ciwjb1srh000ddag3cvexyvzf"},{"post_id":"ciwjb1srb000cdag3dcz8n49e","tag_id":"ciwjb1sqy0007dag3wdnwr1kl","_id":"ciwjb1srm000fdag3x6kn4mcq"},{"post_id":"ciwjb1sri000edag39hompxi7","tag_id":"ciwjb1sqy0007dag3wdnwr1kl","_id":"ciwjb1srq000idag3jvsw6a4o"},{"post_id":"ciwjb1sqz0009dag377iugm43","tag_id":"ciwjb1sra000bdag3z0pszjk9","_id":"ciwjb1srt000kdag309x5wv5p"},{"post_id":"ciwjb1srm000gdag3j1pyoj68","tag_id":"ciwjb1sra000bdag3z0pszjk9","_id":"ciwjb1srx000mdag3gywv11ec"},{"post_id":"ciwjb1sr8000adag3xr0h92ji","tag_id":"ciwjb1sra000bdag3z0pszjk9","_id":"ciwjb1ss0000pdag34mm0x1xc"},{"post_id":"ciwjb1sry000ndag3dgoitpez","tag_id":"ciwjb1sra000bdag3z0pszjk9","_id":"ciwjb1ss3000rdag3o7q1fzw2"},{"post_id":"ciwjb1sru000ldag3wy71j5q4","tag_id":"ciwjb1ss0000odag3to2k8niw","_id":"ciwjb1ssk000vdag3izalx6b1"},{"post_id":"ciwjb1ss1000qdag3ba1w7ijk","tag_id":"ciwjb1sse000tdag39xlyzreo","_id":"ciwjb1ssq000xdag3q9dr7r49"}],"Tag":[{"name":"Python","_id":"ciwjb1sqi0003dag30le4fp9p"},{"name":"Scala","_id":"ciwjb1sqy0007dag3wdnwr1kl"},{"name":"Spark","_id":"ciwjb1sra000bdag3z0pszjk9"},{"name":"Spark DataFrame","_id":"ciwjb1ss0000odag3to2k8niw"},{"name":"Spark Scala Streaming","_id":"ciwjb1sse000tdag39xlyzreo"}]}}